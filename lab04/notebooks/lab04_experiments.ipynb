{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 04: Experiment Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- How experiment management brings observability to ML model development\n",
    "- Which features of experiment management we use in developing the Text Recognizer\n",
    "- Workflows for using Weights & Biases in experiment management, including metric logging, artifact versioning, and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=.:\n",
      ".:\n",
      "/home/terps/.git/fsdl-text-recognizer-2022-labs/lab04\n",
      "\u001b[0m\u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34mtext_recognizer\u001b[0m/  \u001b[01;34mtraining\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "lab_idx = 4\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab contains a large number of embedded iframes\n",
    "that benefit from having a wide window.\n",
    "The cell below makes the notebook as wide as your browser window\n",
    "if `full_width` is set to `True`.\n",
    "Full width is the default behavior in Colab,\n",
    "so this cell is intended to improve the viewing experience in other Jupyter environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow along with a video walkthrough on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"50%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://fsdl.me/2022-lab-04-video-embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdac5aab050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=\"https://fsdl.me/2022-lab-04-video-embed\", width=\"50%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoFCoEcC8SV"
   },
   "source": [
    "# Why experiment management?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why we need experiment management for ML development,\n",
    "let's start by running an experiment.\n",
    "\n",
    "We'll train a new model on a new dataset,\n",
    "using the training script `training/run_experiment.py`\n",
    "introduced in [Lab 02a](https://fsdl.me/lab02a-colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a CNN encoder and Transformer decoder, as in\n",
    "[Lab 03](https://fsdl.me/lab03-colab),\n",
    "but with some changes so we can iterate faster.\n",
    "We'll operate on just single lines of text at a time (`--dataclass IAMLines`), as in\n",
    "[Lab02b](https://fsdl.me/lab02b-colab),\n",
    "and we'll use a smaller CNN (`--modelclass LineCNNTransformer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dataset of images of handwritten text written on a form underneath a typewritten prompt.\n",
      "\n",
      "    \"The IAM Lines dataset, first published at the ICDAR 1999, contains forms of unconstrained handwritten text,\n",
      "    which were scanned at a resolution of 300dpi and saved as PNG images with 256 gray levels.\"\n",
      "    From http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\n",
      "\n",
      "    Images are identified by their \"form ID\". These IDs are used to separate train, validation and test splits,\n",
      "    as keys for dictonaries returning label and image crop region data, and more.\n",
      "\n",
      "    The data split we will use is\n",
      "    IAM lines Large Writer Independent Text Line Recognition Task (LWITLRT): 9,862 text lines.\n",
      "        The validation set has been merged into the train set.\n",
      "        The train set has 7,101 lines from 326 writers.\n",
      "        The test set has 1,861 lines from 128 writers.\n",
      "        The text lines of all data sets are mutually exclusive, thus each writer has contributed to one set only.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from text_recognizer.data.iam import IAM  # base dataset of images of handwritten text\n",
    "from text_recognizer.data import IAMLines  # processed version split into individual lines\n",
    "from text_recognizer.models import LineCNNTransformer  # simple CNN encoder / Transformer decoder\n",
    "\n",
    "\n",
    "print(IAM.__doc__)\n",
    "\n",
    "# uncomment a line below for details on either class\n",
    "# IAMLines??  \n",
    "LineCNNTransformer??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will train a model on 10% of the data for two epochs.\n",
    "\n",
    "It takes up to a few minutes to run on commodity hardware,\n",
    "including data download and preprocessing.\n",
    "As it's running, continue reading below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                      | Type               | Params\n",
      "------------------------------------------------------------------\n",
      "0  | model                     | LineCNNTransformer | 4.3 M \n",
      "1  | model.line_cnn            | LineCNN            | 1.6 M \n",
      "2  | model.embedding           | Embedding          | 21.2 K\n",
      "3  | model.fc                  | Linear             | 21.3 K\n",
      "4  | model.pos_encoder         | PositionalEncoding | 0     \n",
      "5  | model.transformer_decoder | TransformerDecoder | 2.6 M \n",
      "6  | train_acc                 | Accuracy           | 0     \n",
      "7  | val_acc                   | Accuracy           | 0     \n",
      "8  | test_acc                  | Accuracy           | 0     \n",
      "9  | val_cer                   | CharacterErrorRate | 0     \n",
      "10 | test_cer                  | CharacterErrorRate | 0     \n",
      "11 | loss_fn                   | CrossEntropyLoss   | 0     \n",
      "------------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.189    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model State Dict Disk Size: 17.23 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc5a2e875f044e6b068f98192c04d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177ade6a1e0b44f0a8c5168b45a94d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/cer            1.9744949340820312\n",
      "        test/loss            3.235173225402832\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/terps/.git/fsdl-text-recognizer-2022-labs/lab04/training/logs/lightning_logs/version_2/epoch=0000-validation.loss=3.117-validation.cer=1.912.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 25.1 s, total: 1min 43s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "\n",
    "gpus = int(torch.cuda.is_available()) \n",
    "\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 2 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 --limit_test_batches 0.1 --log_every_n_steps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model trains, we're calculating lots of metrics --\n",
    "loss on training and validation, [character error rate](https://torchmetrics.readthedocs.io/en/v0.7.3/references/functional.html#char-error-rate-func) --\n",
    "and reporting them to the terminal.\n",
    "\n",
    "This is achieved by the built-in `.log` method\n",
    "([docs](https://pytorch-lightning.readthedocs.io/en/1.6.1/common/lightning_module.html#train-epoch-level-metrics))\n",
    "of the `LightningModule`,\n",
    "and it is a very straightforward way to get basic information about your experiment as it's running\n",
    "without leaving the context where you're running it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to read\n",
    "[information from streaming numbers in the command line](http://www.quickmeme.com/img/45/4502c7603faf94c0e431761368e9573df164fad15f1bbc27fc03ad493f010dea.jpg)\n",
    "is something of a rite of passage for MLEs, but\n",
    "let's consider what we can't see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're missing all metric values except the most recent --\n",
    "we can see them as they stream in, but they're constantly overwritten.\n",
    "We also can't associate them with timestamps, steps, or epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We also don't see any system metrics.\n",
    "We can't see how much the GPU is being utilized, how much CPU RAM is free, or how saturated our I/O bandwidth is\n",
    "without launching a separate process.\n",
    "And even if we do, those values will also not be saved and timestamped,\n",
    "so we can't correlate them with other things during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we continue to run experiments, changing code and opening new terminals,\n",
    "even the information we have or could figure out now will disappear.\n",
    "Say you spot a weird error message during training,\n",
    "but your session ends and the stdout is gone,\n",
    "so you don't know exactly what it was.\n",
    "Can you recreate the error?\n",
    "Which git branch and commit were you on?\n",
    "Did you have any uncommitted changes? Which arguments did you pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also, model checkpoints containing the parameter values have been saved to disk.\n",
    "Can we relate these checkpoints to their metrics, both in terms of accuracy and in terms of performance?\n",
    "As we run more and more experiments,\n",
    "we'll want to slice and dice them to see if,\n",
    "say, models with `--lr 0.001` are generally better or worse than models with `--lr 0.0001`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save and log all of this information, and more, in order to make our model training\n",
    "[observable](https://docs.honeycomb.io/getting-started/learning-about-observability/) --\n",
    "in short, so that we can understand, make decisions about, and debug our model training\n",
    "by looking at logs and source code, without having to recreate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had to write the logging code we need to save this information ourselves, that'd put us in for a world of hurt:\n",
    "1. That's a lot of code that's not at the core of building an ML-powered system. Robustly saving version control information means becoming _very_ good with your VCS, which is less time spent on mastering the important stuff -- your data, your models, and your problem domain.\n",
    "2. It's very easy to forget to log something that you don't yet realize is going to be critical at some point. Data on netowrk traffic, disk I/O, and GPU/CPU syncing is unimportant until suddenly your training has slowed to a crawl 12 hours into training and you can't figure out where the bottleneck is.\n",
    "3. Once you do start logging everything that's necessary, you might find it's not performant enough -- the code you wrote so you can debug performance issues is [tanking your performance](https://i.imgflip.com/6q54og.jpg).\n",
    "4. Just logging is not enough. The bytes of data need to be made legible to humans in a GUI and searchable via an API, or else they'll be too hard to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Experiment Tracking with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we don't have to. PyTorch Lightning integrates with other libraries for additional logging features,\n",
    "and it makes logging very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.log` method of the `LightningModule` isn't just for logging to the terminal.\n",
    "\n",
    "It can also use a logger to push information elsewhere.\n",
    "\n",
    "By default, we use\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard)\n",
    "via the Lightning `TensorBoardLogger`,\n",
    "which has been saving results to the local disk.\n",
    "\n",
    "Let's find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a sequence of bash commands to get the latest experiment's directory\n",
    "#  by hand, you can just copy and paste it from the terminal\n",
    "\n",
    "list_all_log_files = \"find training/logs/lightning_logs/\"  # find avoids issues ls has with \\n in filenames\n",
    "filter_to_folders = \"grep '_[0-9]*$'\"  # regex match on end of line\n",
    "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
    "take_first = \"head -n 1\"  # the first n elements, n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training/logs/lightning_logs/version_2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_log, = ! {list_all_log_files} | {filter_to_folders} | {sort_version_descending} | {take_first}\n",
    "latest_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 99M\r\n",
      "-rw-r--r-- 1 terps terps  50M Oct  3 11:49 'epoch=0000-validation.loss=3.117-validation.cer=1.912.ckpt'\r\n",
      "-rw-r--r-- 1 terps terps  50M Oct  3 11:50 'epoch=0001-validation.loss=3.117-validation.cer=1.912.ckpt'\r\n",
      "-rw-r--r-- 1 terps terps 1.3K Oct  3 11:50  events.out.tfevents.1664815750.ThePortal.4575.0\r\n",
      "-rw-r--r-- 1 terps terps  176 Oct  3 11:50  events.out.tfevents.1664815843.ThePortal.4575.1\r\n",
      "-rw-r--r-- 1 terps terps    3 Oct  3 11:49  hparams.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {latest_log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view results, we need to launch a TensorBoard server --\n",
    "much like we need to launch a Jupyter server to use Jupyter notebooks.\n",
    "\n",
    "The cells below load an extension that lets you use TensorBoard inside of a notebook\n",
    "the same way you'd use it from the command line, and then launch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b546b9173c3e4732\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b546b9173c3e4732\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 11717;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# same command works in terminal, with \"{arguments}\" replaced with values or \"$VARIABLES\"\n",
    "\n",
    "port = 11717  # pick an open port on your machine\n",
    "host = \"0.0.0.0\" # allow connections from the internet\n",
    "                 #   watch out! make sure you turn TensorBoard off\n",
    "\n",
    "%tensorboard --logdir {latest_log} --port {port} --host {host}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some charts of metrics over time along with some charting controls.\n",
    "\n",
    "You can click around in this interface and explore it if you'd like,\n",
    "but in the next section, we'll see that there are better tools for experiment management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've run many experiments on this machine,\n",
    "you can see all of their results by pointing TensorBoard\n",
    "at the whole `lightning_logs` directory,\n",
    "rather than just one experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7df5a044d85c99e2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7df5a044d85c99e2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 11718;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir training/logs/lightning_logs --port {port + 1} --host \"0.0.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large numbers of experiments, the management experience is not great --\n",
    "it's for example hard to go from a line in a chart to metadata about the experiment or metric depicted in tha tline.\n",
    "\n",
    "It's especially difficult to switch between types of experiments, to compare experiments run on different machines, or to collaborate with others,\n",
    "which are important workflows as applications mature and teams grow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard is an independent service, so we need to make sure we turn it off when we're done. Just flip `done_with_tensorboard` to `True`.\n",
    "\n",
    "If you run into any issues with the above cells failing to launch,\n",
    "especially across iterations of this lab, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard.manager\n",
    "\n",
    "# get the process IDs for all tensorboard instances\n",
    "pids = [tb.pid for tb in tensorboard.manager.get_all()]\n",
    "\n",
    "done_with_tensorboard = True\n",
    "\n",
    "if done_with_tensorboard:\n",
    "    # kill processes\n",
    "    for pid in pids:\n",
    "        !kill {pid} 2> /dev/null\n",
    "        \n",
    "    # remove the temporary files that sometimes persist, see https://stackoverflow.com/a/59582163\n",
    "    !rm -rf {tensorboard.manager._get_info_dir()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Management with Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we manage experiments when we hit the limits of local TensorBoard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is powerful and flexible and very scalable,\n",
    "but running it requires engineering effort and babysitting --\n",
    "you're running a database, writing data to it,\n",
    "and layering a web application over it it.\n",
    "\n",
    "This is a fairly common workflow for web developers,\n",
    "but not so much for ML engineers.\n",
    "\n",
    "You can avoid this with [tensorboard.dev](https://tensorboard.dev/),\n",
    "and it's as simple as running the command `tensorboard dev upload`\n",
    "pointed at your logging directory.\n",
    "\n",
    "But there are strict limits to this free service:\n",
    "1GB of tensor data and 1GB of binary data.\n",
    "A single Text Recognizer compiled model checkpoint is ~500MB,\n",
    "and that's not particularly large for a useful model.\n",
    "\n",
    "Furthermore, all data is public,\n",
    "so if you upload the inputs and outputs of your model,\n",
    "anyone who finds the link can see them.\n",
    "\n",
    "Overall, tensorboard.dev works very well for certain academic and open projects\n",
    "but not for industrial ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid that narrow permissions and limits issue,\n",
    "you could use [git LFS](https://git-lfs.github.com/)\n",
    "to track the binary data and tensor data,\n",
    "which is more likely to be sensitive than metrics.\n",
    "\n",
    "The Hugging Face ecosystem uses TensorBoard and git LFS.\n",
    "\n",
    "The Hugging Face Hub, a git server much like GitHub,\n",
    "[will host TensorBoard alongside models](https://huggingface.co/docs/hub/tensorboard)\n",
    "and officially has\n",
    "[no storage limit](https://discuss.huggingface.co/t/is-there-a-size-limit-for-dataset-hosting/14861/4),\n",
    "avoiding the\n",
    "[tight bandwidth and storage limits](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage),\n",
    "that make using git LFS with GitHub infeasible.\n",
    "\n",
    "Using the hub requires maintaining an additional git remote or switching from GitHub,\n",
    "which is a hard ask for most engineering teams.\n",
    "\n",
    "And furthermore, this separation is un-natural:\n",
    "model binaries, metrics,\n",
    "and tracked inputs/outputs\n",
    "are all needed to debug model development pipelines,\n",
    "and fragmenting them across services makes debugging harder.\n",
    "\n",
    "Additionally, git-style versioning is an awkward fit for logging --\n",
    "is it really sensible to create a new commit for each logging event while you're watching live?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple alternatives to TensorBoard.\n",
    "The primary [open governance](https://www.ibm.com/blogs/cloud-computing/2016/10/27/open-source-open-governance/)\n",
    "tool is [MLflow](https://github.com/mlflow/mlflow/)\n",
    "and there are a number of\n",
    "[closed-governance and/or closed-source tools](https://www.reddit.com/r/MachineLearning/comments/q5g7m9/n_sagemaker_experiments_vs_comet_neptune_wandb_etc/).\n",
    "\n",
    "These tools generally avoid any need to worry about hosting\n",
    "(unless data governance rules require a self-hosted version).\n",
    "\n",
    "For a sampling of publicly-posted opinions on experiment management tools,\n",
    "see these discussions from Reddit:\n",
    "\n",
    "- r/mlops: [1](https://www.reddit.com/r/mlops/comments/uxieq3/is_weights_and_biases_worth_the_money/), [2](https://www.reddit.com/r/mlops/comments/sbtkxz/best_mlops_platform_for_2022/)\n",
    "- r/MachineLearning: [3](https://www.reddit.com/r/MachineLearning/comments/sqa36p/comment/hwls9px/?utm_source=share&utm_medium=web2x&context=3)\n",
    "\n",
    "Among these tools, the FSDL recommendation is\n",
    "[Weights & Biases](https://wandb.ai),\n",
    "which we believe offers\n",
    "- the best user experience, both in the Python SDKs and in the graphical interface\n",
    "- the best integrations with other tools,\n",
    "including\n",
    "[Lightning](https://docs.wandb.ai/guides/integrations/lightning) and\n",
    "[Keras](https://docs.wandb.ai/guides/integrations/keras),\n",
    "[Jupyter](https://docs.wandb.ai/guides/track/jupyter),\n",
    "and even\n",
    "[TensorBoard](https://docs.wandb.ai/guides/integrations/tensorboard),\n",
    "and\n",
    "- the best tools for collaboration.\n",
    "\n",
    "Below, we'll take care to point out which logging and management features\n",
    "are available via generic interfaces in Lightning and which are W&B-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use wandb to track machine learning work.\n",
      "\n",
      "The most commonly used functions/objects are:\n",
      "  - wandb.init — initialize a new run at the top of your training script\n",
      "  - wandb.config — track hyperparameters and metadata\n",
      "  - wandb.log — log metrics and media over time within your training loop\n",
      "\n",
      "For guides and examples, see https://docs.wandb.com/guides.\n",
      "\n",
      "For scripts and interactive notebooks, see https://github.com/wandb/examples.\n",
      "\n",
      "For reference documentation, see https://docs.wandb.com/ref/python.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "print(wandb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding it to our experiment running code is extremely easy,\n",
    "relative to the features we get, which is\n",
    "one of the main selling points of W&B.\n",
    "\n",
    "We get most of our new experiment management features just by changing a single variable, `logger`, from\n",
    "`TensorboardLogger` to `WandbLogger`\n",
    "and adding two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    if args.wandb:\r\n",
      "        logger = pl.loggers.WandbLogger(log_model=\"all\", save_dir=str(log_dir), job_type=\"train\")\r\n",
      "        logger.watch(model, log_freq=max(100, args.log_every_n_steps))\r\n",
      "        logger.log_hyperparams(vars(args))\r\n",
      "        experiment_dir = logger.experiment.dir\r\n",
      "    callbacks += [cb.ModelSizeLogger(), cb.LearningRateMonitor()]\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"args.wandb\" -A 5 training/run_experiment.py | head -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see what each of these lines does for us below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this logger is built into and maintained by PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "WandbLogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the rest of this notebook,\n",
    "you'll need a Weights & Biases account.\n",
    "\n",
    "As with GitHub the free tier, for personal, academic, and open source work,\n",
    "is very generous.\n",
    "\n",
    "The Text Recognizer project will fit comfortably within the free tier.\n",
    "\n",
    "Run the cell below and follow the prompts to log in or create an account or go\n",
    "[here](https://wandb.ai/signup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterps\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to launch an experiment tracked with Weights & Biases.\n",
    "\n",
    "The experiment can take between 3 and 10 minutes to run.\n",
    "In that time, continue reading below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>training/logs/wandb/run-20221003_115047-qee9gexk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/runs/qee9gexk\" target=\"_blank\">lively-microwave-2</a></strong> to <a href=\"https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                      | Type               | Params\n",
      "------------------------------------------------------------------\n",
      "0  | model                     | LineCNNTransformer | 4.3 M \n",
      "1  | model.line_cnn            | LineCNN            | 1.6 M \n",
      "2  | model.embedding           | Embedding          | 21.2 K\n",
      "3  | model.fc                  | Linear             | 21.3 K\n",
      "4  | model.pos_encoder         | PositionalEncoding | 0     \n",
      "5  | model.transformer_decoder | TransformerDecoder | 2.6 M \n",
      "6  | train_acc                 | Accuracy           | 0     \n",
      "7  | val_acc                   | Accuracy           | 0     \n",
      "8  | test_acc                  | Accuracy           | 0     \n",
      "9  | val_cer                   | CharacterErrorRate | 0     \n",
      "10 | test_cer                  | CharacterErrorRate | 0     \n",
      "11 | loss_fn                   | CrossEntropyLoss   | 0     \n",
      "------------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.189    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model State Dict Disk Size: 17.23 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cd93d6216c4d3586a67212ce71df20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccc97395e9a40f3aba399ef65891c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/cer            1.7503788471221924\n",
      "        test/loss            2.509263277053833\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/terps/.git/fsdl-text-recognizer-2022-labs/lab04/training/logs/lightning_logs/version_3/epoch=0008-validation.loss=2.421-validation.cer=0.855.ckpt\n",
      "Best model also uploaded to W&B \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='526.851 MB of 526.851 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>optimizer/lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>size/mb_disk</td><td>▁</td></tr><tr><td>size/nparams</td><td>▁</td></tr><tr><td>test/cer</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▇▇▇▇▇██▆▄▄▃▃▃▂▂▂▂▁▂▂▂▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>validation/cer</td><td>████▇▇▁▇▁▆</td></tr><tr><td>validation/loss</td><td>████▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>optimizer/lr-Adam</td><td>0.001</td></tr><tr><td>size/mb_disk</td><td>17.22701</td></tr><tr><td>size/nparams</td><td>4297331</td></tr><tr><td>test/cer</td><td>1.75038</td></tr><tr><td>test/loss</td><td>2.50926</td></tr><tr><td>train/loss</td><td>2.47715</td></tr><tr><td>trainer/global_step</td><td>290</td></tr><tr><td>validation/cer</td><td>1.67365</td></tr><tr><td>validation/loss</td><td>2.39395</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lively-microwave-2</strong>: <a href=\"https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/runs/qee9gexk\" target=\"_blank\">https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/runs/qee9gexk</a><br/>Synced 6 W&B file(s), 41 media file(s), 1330 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>training/logs/wandb/run-20221003_115047-qee9gexk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 58s, sys: 1min 27s, total: 6min 25s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 10 \\\n",
    "  --log_every_n_steps 10 --wandb --limit_test_batches 0.1 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1\n",
    "    \n",
    "last_expt = wandb.run\n",
    "\n",
    "wandb.finish()  # necessary in this style of in-notebook experiment running, not necessary in CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some new things in our output.\n",
    "\n",
    "For example, there's a note from `wandb` that the data is saved locally\n",
    "and also synced to their servers.\n",
    "\n",
    "There's a link to a webpage for viewing the logged data and a name for our experiment --\n",
    "something like `dandy-sunset-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local logging and cloud syncing happens with minimal impact on performance,\n",
    "because `wandb` launches a separate process to listen for events and upload them.\n",
    "\n",
    "That's a table-stakes feature for a logging framework but not a pleasant thing to write in Python yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view results, head to the link in the notebook output\n",
    "that looks like \"Syncing run **{adjective}-{noun}-{number}**\".\n",
    "\n",
    "There's no need to wait for training to finish.\n",
    "\n",
    "The next sections describe the contents of that interface. You can read them while looking at the W&B interface in a separate tab or window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even more convenience, once training is finished we can also see the results directly in the notebook by embedding the webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/runs/qee9gexk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/runs/qee9gexk\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda60997e90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(last_expt.url)\n",
    "IFrame(last_expt.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have landed on the run page\n",
    "([docs](https://docs.wandb.ai/ref/app/pages/run-page)),\n",
    "which collects up all of the information for a single experiment into a collection of tabs.\n",
    "\n",
    "We'll work through these tabs from top to bottom.\n",
    "\n",
    "Each header is also a link to the documentation for a tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Overview tab](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab)\n",
    "This tab has an icon that looks like `(i)` or 🛈.\n",
    "\n",
    "The top section of this tab has high-level information about our run:\n",
    "- Timing information, like start time and duration\n",
    "- System hardware, hostname, and basic environment info\n",
    "- Git repository link and state\n",
    "\n",
    "This information is collected and logged automatically.\n",
    "\n",
    "The section at the bottom contains configuration information, which here includes all CLI args or their defaults,\n",
    "and summary metrics.\n",
    "\n",
    "Configuration information is collected with `.log_hyperparams` in Lightning or `wandb.config` otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Charts tab](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab)\n",
    "\n",
    "This tab has a line plot icon, something like 📈.\n",
    "\n",
    "It's also the default page you land on when looking at a W&B run.\n",
    "\n",
    "Charts are generated for everything we `.log` from PyTorch Lightning. The charts here are interactive and editable, and changes persist.\n",
    "\n",
    "Unfurl the \"Gradients\" section in this tab to check out the gradient histograms. These histograms can be useful for debugging training instability issues.\n",
    "\n",
    "We were able to log these just by calling `wandb.watch` on our model. This is a W&B-specific feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [System tab](https://docs.wandb.ai/ref/app/pages/run-page#system-tab)\n",
    "This tab has computer chip icon.\n",
    "\n",
    "It contains\n",
    "- GPU metrics for all GPUs: temperature, [utilization](https://stackoverflow.com/questions/5086814/how-is-gpu-and-memory-utilization-defined-in-nvidia-smi-results), and memory allocation\n",
    "- CPU metrics: memory usage, utilization, thread counts\n",
    "- Disk and network I/O levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model tab](https://docs.wandb.ai/ref/app/pages/run-page#model-tab)\n",
    "This tab has an undirected graph icon that looks suspiciously like a [pawnbrokers' symbol](https://en.wikipedia.org/wiki/Pawnbroker#:~:text=The%20pawnbrokers%27%20symbol%20is%20three,the%20name%20of%20Lombard%20banking.).\n",
    "\n",
    "The information here was also generated from `wandb.watch`, and includes parameter counts and input/output shapes for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Logs tab](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab)\n",
    "This tab has an icon that looks like a stylized command prompt, `>_`.\n",
    "\n",
    "It contains information that was printed to the stdout.\n",
    "\n",
    "This tab is useful for, e.g., determining when exactly a warning or error message started appearing.\n",
    "\n",
    "Note that model summary information is printed here. We achieve this with a Lightning `Callback` called `ModelSummary`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    summary_callback = pl.callbacks.ModelSummary(max_depth=2)\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"callbacks.ModelSummary\" training/run_experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning `Callback`s add extra \"nice-to-have\" engineering features to our model training.\n",
    "\n",
    "For more on Lightning `Callback`s, see\n",
    "[Lab 02a](https://fsdl.me/lab02a-colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Files tab](https://docs.wandb.ai/ref/app/pages/run-page#files-tab)\n",
    "This tab has a stylized document icon, something like 📄.\n",
    "\n",
    "You can use this tab to view any files saved with the `wandb.save`.\n",
    "\n",
    "For most uses, that style is deprecated in favor of `wandb.log_artifact`,\n",
    "which we'll discuss shortly.\n",
    "\n",
    "But a few pieces of information automatically collected by W&B end up in this tab.\n",
    "\n",
    "Some highlights:\n",
    "  - Much more detailed environment info: `conda-environment.yaml` and `requirements.txt`\n",
    "  - A `diff.patch` that represents the difference between the files in the `git` commit logged in the overview and the actual disk state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Artifacts tab](https://docs.wandb.ai/ref/app/pages/run-page#artifacts-tab)\n",
    "This tab has the database or [drum memory icon](https://stackoverflow.com/a/2822750), which looks like a cylinder of three stacked hockey pucks.\n",
    "\n",
    "This tab contains all of the versioned binary files, aka artifacts, associated with our run.\n",
    "\n",
    "We store two kinds of binary files\n",
    "  - `run_table`s of model inputs and outputs\n",
    "  - `model` checkpoints\n",
    "\n",
    "We get model checkpoints via the built-in Lightning `ModelCheckpoint` callback, which is not specific to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\r\n",
      "        save_top_k=5,\r\n",
      "        filename=filename_format,\r\n",
      "        monitor=goldstar_metric,\r\n",
      "        mode=\"min\",\r\n",
      "        auto_insert_metric_name=False,\r\n",
      "        dirpath=experiment_dir,\r\n",
      "        every_n_epochs=args.check_val_every_n_epoch,\r\n",
      "    )\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"callbacks.ModelCheckpoint\" -A 9 training/run_experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools for working with artifacts in W&B are powerful and complex, so we'll cover them in various places throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Tables of Logged Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the Charts tab,\n",
    "notice that we have model inputs and outputs logged in structured tables\n",
    "under the train, validation, and test sections.\n",
    "\n",
    "These tables are interactive as well\n",
    "([docs](https://docs.wandb.ai/guides/data-vis/log-tables)).\n",
    "They support basic exploratory data analysis and are compatible with W&B's collaboration features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to charts in our run page, these tables also have their own pages inside the W&B web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/artifacts/run_table/run-qee9gexk-trainpredictions/v0/files/train/predictions.table.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/artifacts/run_table/run-qee9gexk-trainpredictions/v0/files/train/predictions.table.json\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda60191790>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_versions_url = last_expt.url.split(\"runs\")[0] + f\"artifacts/run_table/run-{last_expt.id}-trainpredictions/\"\n",
    "table_data_url = table_versions_url + \"v0/files/train/predictions.table.json\"\n",
    "\n",
    "print(table_data_url)\n",
    "IFrame(src=table_data_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting this to work requires more effort and more W&B-specific code\n",
    "than the other features we've seen so far.\n",
    "\n",
    "We'll briefly explain the implementation here, for those who are interested.\n",
    "\n",
    "We use a custom Lightning `Callback`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.callbacks.imtotext import ImageToTextTableLogger\n",
    "\n",
    "\n",
    "ImageToTextTableLogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Lightning returns logged information on every batch and these outputs are accumulated throughout an epoch.\n",
    "\n",
    "The values are then aggregated with a frequency determined by the `pl.Trainer` argument `--log_every_n_batches`.\n",
    "\n",
    "This behavior is sensible for metrics, which are low overhead, but not so much for media,\n",
    "where we'd rather subsample and avoid holding on to too much information.\n",
    "\n",
    "So we additionally control when media is included in the outputs with methods like `add_on_logged_batches`.\n",
    "\n",
    "The frequency of media logging is then controlled with `--log_every_n_batches`, as with aggregate metric reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models.base import BaseImageToTextLitModel\n",
    "\n",
    "BaseImageToTextLitModel.add_on_logged_batches??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we've seen so far has been related to a single run or experiment.\n",
    "\n",
    "Experiment management starts to shine when you can organize, filter, and group many experiments at once.\n",
    "\n",
    "We organize our runs into \"projects\" and view them on the  W&B \"project page\" \n",
    "([docs](https://docs.wandb.ai/ref/app/pages/project-page)).\n",
    "\n",
    "By default in the Lightning integration, the project name is determined based on directory information.\n",
    "This default can be over-ridden in the code when creating a `WandbLogger`,\n",
    "but we find it easier to change it from the command line by setting the `WANDB_PROJECT` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the project page looks like for a longer-running project with lots of experiments\n",
    "\n",
    "The cell below pulls up the project page for some of the debugging and feature addition work done while updating the course from 2021 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/workspace\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/workspace\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda605058d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/workspace\"\n",
    "\n",
    "print(project_url)\n",
    "IFrame(src=project_url, width=\"100%\", height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page and these charts have been customized -- filtering down to the most interesting training runs and surfacing the most import high-level information about them.\n",
    "\n",
    "We welcome you to poke around in this interface: deactivate or change the filters, clicking through into individual runs, and change the charts around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond logging metrics and metadata from runs,\n",
    "we can also log and version large binary files, or artifacts, and their metadata ([docs](https://docs.wandb.ai/guides/artifacts/artifacts-core-concepts))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below pulls up all of the artifacts associated with the experiment we just ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/terps/fsdl-text-recognizer-2022-labs-lab04/runs/qee9gexk/artifacts\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda6026b1d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=last_expt.url + \"/artifacts\", width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on one of the `model` checkpoints -- the specific version doesn't matter.\n",
    "\n",
    "There are a number of tabs here.\n",
    "\n",
    "The \"Overview\" tab includes automatically generated metadata, like which run by which user created this model checkpoint, when, and how much disk space it takes up.\n",
    "\n",
    "The \"Metadata\" tab includes configurable metadata, here hyperparameters and metrics like `validation/cer`,\n",
    "which are added by default by the `WandbLogger`.\n",
    "\n",
    "The \"Files\" tab contains the actual file contents of the artifact.\n",
    "\n",
    "On the left-hand side of the page, you'll see the other versions of the model checkpoint,\n",
    "including some versions that are \"tagged\" with version aliases, like `latest` or `best`.\n",
    "\n",
    "You can click on these to explore the different versions and even directly compare them.\n",
    "\n",
    "If you're particularly interested in this tool, try comparing two versions of the `validation-predictions` artifact, starting from the Files tab and clicking inside it to `validation/predictions.table.json`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artifact storage is part of the W&B free tier.\n",
    "\n",
    "The storage limits, as of August 2022, cover 100GB of Artifacts and experiment data.\n",
    "\n",
    "The former is sufficient to store ~700 model checkpoints for the Text Recognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your data storage and compare it to your limits at this URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/usage/terps\n"
     ]
    }
   ],
   "source": [
    "storage_tracker_url = f\"https://wandb.ai/usage/{last_expt.entity}\"\n",
    "\n",
    "print(storage_tracker_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also programmatically access our data and metadata via the `wandb` API\n",
    "([docs](https://docs.wandb.ai/guides/track/public-api-guide)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can access the metrics we just logged as a `pandas.DataFrame` by grabbing the run via the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainer/global_step</th>\n",
       "      <th>_step</th>\n",
       "      <th>_runtime</th>\n",
       "      <th>size/nparams</th>\n",
       "      <th>size/mb_disk</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>validation/predictions</th>\n",
       "      <th>optimizer/lr-Adam</th>\n",
       "      <th>train/predictions</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>...</th>\n",
       "      <th>gradients/transformer_decoder.layers.0.norm3.bias</th>\n",
       "      <th>gradients/transformer_decoder.layers.2.self_attn.in_proj_weight</th>\n",
       "      <th>gradients/transformer_decoder.layers.1.self_attn.in_proj_weight</th>\n",
       "      <th>gradients/transformer_decoder.layers.0.linear1.weight</th>\n",
       "      <th>gradients/line_cnn.convs.2.conv.weight</th>\n",
       "      <th>gradients/transformer_decoder.layers.1.norm3.weight</th>\n",
       "      <th>gradients/transformer_decoder.layers.1.self_attn.out_proj.weight</th>\n",
       "      <th>gradients/transformer_decoder.layers.1.norm2.weight</th>\n",
       "      <th>test/cer</th>\n",
       "      <th>test/loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4297331.0</td>\n",
       "      <td>17.22701</td>\n",
       "      <td>1664815863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1664815868</td>\n",
       "      <td>{'artifact_path': 'wandb-client-artifact://16q...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1664815872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1664815874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1664815877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'artifact_path': 'wandb-client-artifact://dhl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trainer/global_step  _step  _runtime  size/nparams  size/mb_disk  \\\n",
       "0                   -1      0        16     4297331.0      17.22701   \n",
       "1                   -1      1        21           NaN           NaN   \n",
       "2                    0      2        25           NaN           NaN   \n",
       "3                    9      3        27           NaN           NaN   \n",
       "4                    9      4        30           NaN           NaN   \n",
       "\n",
       "   _timestamp                             validation/predictions  \\\n",
       "0  1664815863                                                NaN   \n",
       "1  1664815868  {'artifact_path': 'wandb-client-artifact://16q...   \n",
       "2  1664815872                                                NaN   \n",
       "3  1664815874                                                NaN   \n",
       "4  1664815877                                                NaN   \n",
       "\n",
       "   optimizer/lr-Adam                                  train/predictions  \\\n",
       "0                NaN                                                NaN   \n",
       "1                NaN                                                NaN   \n",
       "2              0.001                                                NaN   \n",
       "3              0.001                                                NaN   \n",
       "4                NaN  {'artifact_path': 'wandb-client-artifact://dhl...   \n",
       "\n",
       "   train/loss  ...  gradients/transformer_decoder.layers.0.norm3.bias  \\\n",
       "0         NaN  ...                                                NaN   \n",
       "1         NaN  ...                                                NaN   \n",
       "2         NaN  ...                                                NaN   \n",
       "3         NaN  ...                                                NaN   \n",
       "4         NaN  ...                                                NaN   \n",
       "\n",
       "   gradients/transformer_decoder.layers.2.self_attn.in_proj_weight  \\\n",
       "0                                                NaN                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "   gradients/transformer_decoder.layers.1.self_attn.in_proj_weight  \\\n",
       "0                                                NaN                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "  gradients/transformer_decoder.layers.0.linear1.weight  \\\n",
       "0                                                NaN      \n",
       "1                                                NaN      \n",
       "2                                                NaN      \n",
       "3                                                NaN      \n",
       "4                                                NaN      \n",
       "\n",
       "  gradients/line_cnn.convs.2.conv.weight  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "  gradients/transformer_decoder.layers.1.norm3.weight  \\\n",
       "0                                                NaN    \n",
       "1                                                NaN    \n",
       "2                                                NaN    \n",
       "3                                                NaN    \n",
       "4                                                NaN    \n",
       "\n",
       "  gradients/transformer_decoder.layers.1.self_attn.out_proj.weight  \\\n",
       "0                                                NaN                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "  gradients/transformer_decoder.layers.1.norm2.weight test/cer test/loss  \n",
       "0                                                NaN       NaN       NaN  \n",
       "1                                                NaN       NaN       NaN  \n",
       "2                                                NaN       NaN       NaN  \n",
       "3                                                NaN       NaN       NaN  \n",
       "4                                                NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wb_api.run(\"/\".join( # fetch a run given\n",
    "    [last_expt.entity,     # the user or org it was logged to\n",
    "     last_expt.project,    # the \"project\", usually one of several per repo/application\n",
    "     last_expt.id]         # and a unique ID\n",
    "))\n",
    "\n",
    "hist = run.history()  # and pull down a sample of the data as a pandas DataFrame\n",
    "\n",
    "hist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch\n",
       "0.0     3.298734\n",
       "1.0     3.213943\n",
       "2.0     3.216021\n",
       "3.0     3.237335\n",
       "4.0     3.071682\n",
       "5.0     2.770668\n",
       "6.0     2.655726\n",
       "7.0     2.598666\n",
       "8.0     2.576380\n",
       "9.0     2.536892\n",
       "10.0         NaN\n",
       "Name: train/loss, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.groupby(\"epoch\")[\"train/loss\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this includes the artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact of type run_table: run-qee9gexk-validationpredictions:v0\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v0\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v1\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v1\n",
      "artifact of type model: model-qee9gexk:v0\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v2\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v3\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v4\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v2\n",
      "artifact of type model: model-qee9gexk:v1\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v5\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v6\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v7\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v3\n",
      "artifact of type model: model-qee9gexk:v2\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v8\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v9\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v10\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v4\n",
      "artifact of type model: model-qee9gexk:v3\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v11\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v12\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v13\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v5\n",
      "artifact of type model: model-qee9gexk:v4\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v14\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v15\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v16\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v6\n",
      "artifact of type model: model-qee9gexk:v5\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v17\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v18\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v19\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v7\n",
      "artifact of type model: model-qee9gexk:v6\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v20\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v21\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v22\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v8\n",
      "artifact of type model: model-qee9gexk:v7\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v23\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v24\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v25\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v9\n",
      "artifact of type model: model-qee9gexk:v8\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v26\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v27\n",
      "artifact of type run_table: run-qee9gexk-trainpredictions:v28\n",
      "artifact of type run_table: run-qee9gexk-validationpredictions:v10\n",
      "artifact of type model: model-qee9gexk:v9\n"
     ]
    }
   ],
   "source": [
    "# which artifacts where created and logged?\n",
    "artifacts = run.logged_artifacts()\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(f\"artifact of type {artifact.type}: {artifact.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our `ImageToTextTableLogger`,\n",
    "we can easily recreate training or validation data that came out of our `DataLoader`s,\n",
    "which is normally ephemeral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "artifact = wb_api.artifact(f\"{last_expt.entity}/{last_expt.project}/run-{last_expt.id}-trainpredictions:latest\")\n",
    "artifact_dir = Path(artifact.download(root=\"training/logs\"))\n",
    "image_dir = artifact_dir / \"media\" / \"images\"\n",
    "\n",
    "images = [path for path in image_dir.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(str(random.choice(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced W&B API Usage: MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of a well-instrumented experiment tracking system is that it allows\n",
    "automatic relation of information:\n",
    "what were the inputs when this model's gradient spiked?\n",
    "which models have been trained on this dataset,\n",
    "and what was their performance?\n",
    "\n",
    "Having access and automation around this information is necessary for \"MLOps\",\n",
    "which applies contemporary DevOps principles to ML projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below pull down the training data\n",
    "for the model currently running the FSDL Text Recognizer app.\n",
    "\n",
    "This is just intended as a demonstration of what's possible,\n",
    "so don't worry about understanding every piece of this,\n",
    "and feel free to skip past it.\n",
    "\n",
    "MLOps is still a nascent field, and these tools and workflows are likely to change.\n",
    "\n",
    "For example, just before the course launched, W&B released a\n",
    "[Model Registry layer](https://docs.wandb.ai/guides/models)\n",
    "on top of artifact logging that aims to improve the developer experience for these workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the same project we looked at in the project view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/workspace?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<Project cfrye59/fsdl-text-recognizer-2021-training>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_recognizer_project = wb_api.project(\"fsdl-text-recognizer-2021-training\", entity=\"cfrye59\")\n",
    "\n",
    "text_recognizer_project  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we search it for the text recognizer model currently being used in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/artifacts/prod-ready/paragraph-text-recognizer/v8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/artifacts/prod-ready/paragraph-text-recognizer/v8\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda32d1a190>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all versions of the text-recognizer ever put into production by...\n",
    "\n",
    "for art_type in text_recognizer_project.artifacts_types(): # looking through all artifact types\n",
    "    if art_type.name == \"prod-ready\":  # for the prod-ready type\n",
    "        # and grabbing the text-recognizer\n",
    "        production_text_recognizers = art_type.collection(\"paragraph-text-recognizer\").versions()\n",
    "\n",
    "# and then get the one that's currently being tested in CI by...\n",
    "for text_recognizer in production_text_recognizers:\n",
    "    if \"ci-test\" in text_recognizer.aliases:  # looking for the one that's labeled as CI-tested\n",
    "        in_prod_text_recognizer = text_recognizer\n",
    "\n",
    "# view its metadata at the url or in the notebook\n",
    "in_prod_text_recognizer_url = text_recognizer_project.url[:-9] + f\"artifacts/{in_prod_text_recognizer.type}/{in_prod_text_recognizer.name.replace(':', '/')}\"\n",
    "\n",
    "print(in_prod_text_recognizer_url)\n",
    "IFrame(src=in_prod_text_recognizer_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From its metadata, we can get information about how it was \"staged\" to be put into production,\n",
    "and in particular which model checkpoint was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model-1vrnrd8p:v41'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_run = in_prod_text_recognizer.logged_by()\n",
    "\n",
    "training_ckpt, = [at for at in staging_run.used_artifacts() if at.type == \"model\"]\n",
    "training_ckpt.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That checkpoint was logged by a training experiment, which is available as metadata.\n",
    "\n",
    "We can look at the training run for that model, either here in the notebook or at its URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/runs/1vrnrd8p\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/runs/1vrnrd8p\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda32c7add0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_run = training_ckpt.logged_by()\n",
    "print(training_run.url)\n",
    "IFrame(src=training_run.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, we can access logs and metadata about training,\n",
    "confident that we are working with the model that is actually in production.\n",
    "\n",
    "For example, we can pull down the data we logged and analyze it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainer/global_step</th>\n",
       "      <th>_step</th>\n",
       "      <th>_runtime</th>\n",
       "      <th>size/nparams</th>\n",
       "      <th>size/mb_disk</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>validation/predictions</th>\n",
       "      <th>optimizer/lr-Adam</th>\n",
       "      <th>gradients/encoder_projection.weight</th>\n",
       "      <th>gradients/transformer_decoder.layers.0.linear2.bias</th>\n",
       "      <th>...</th>\n",
       "      <th>gradients/transformer_decoder.layers.1.self_attn.out_proj.weight</th>\n",
       "      <th>gradients/resnet.4.1.bn2.bias</th>\n",
       "      <th>gradients/transformer_decoder.layers.1.norm2.weight</th>\n",
       "      <th>train/predictions</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>validation/cer</th>\n",
       "      <th>validation/loss</th>\n",
       "      <th>test/cer</th>\n",
       "      <th>test/loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>13988756.0</td>\n",
       "      <td>56.064631</td>\n",
       "      <td>1654129282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1654129317</td>\n",
       "      <td>{'size': 34810, '_latest_artifact_path': 'wand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1654129317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1654129343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1654129361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trainer/global_step  _step  _runtime  size/nparams  size/mb_disk  \\\n",
       "0                   -1      0        98    13988756.0     56.064631   \n",
       "1                   -1      1       133           NaN           NaN   \n",
       "2                    0      2       133           NaN           NaN   \n",
       "3                   39      3       159           NaN           NaN   \n",
       "4                   78      4       177           NaN           NaN   \n",
       "\n",
       "   _timestamp                             validation/predictions  \\\n",
       "0  1654129282                                                NaN   \n",
       "1  1654129317  {'size': 34810, '_latest_artifact_path': 'wand...   \n",
       "2  1654129317                                                NaN   \n",
       "3  1654129343                                                NaN   \n",
       "4  1654129361                                                NaN   \n",
       "\n",
       "   optimizer/lr-Adam gradients/encoder_projection.weight  \\\n",
       "0                NaN                                 NaN   \n",
       "1                NaN                                 NaN   \n",
       "2             0.0001                                 NaN   \n",
       "3             0.0001                                 NaN   \n",
       "4             0.0001                                 NaN   \n",
       "\n",
       "  gradients/transformer_decoder.layers.0.linear2.bias  ...  \\\n",
       "0                                                NaN   ...   \n",
       "1                                                NaN   ...   \n",
       "2                                                NaN   ...   \n",
       "3                                                NaN   ...   \n",
       "4                                                NaN   ...   \n",
       "\n",
       "  gradients/transformer_decoder.layers.1.self_attn.out_proj.weight  \\\n",
       "0                                                NaN                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "  gradients/resnet.4.1.bn2.bias  \\\n",
       "0                           NaN   \n",
       "1                           NaN   \n",
       "2                           NaN   \n",
       "3                           NaN   \n",
       "4                           NaN   \n",
       "\n",
       "  gradients/transformer_decoder.layers.1.norm2.weight train/predictions  \\\n",
       "0                                                NaN                NaN   \n",
       "1                                                NaN                NaN   \n",
       "2                                                NaN                NaN   \n",
       "3                                                NaN                NaN   \n",
       "4                                                NaN                NaN   \n",
       "\n",
       "  train/loss epoch validation/cer validation/loss test/cer test/loss  \n",
       "0        NaN   NaN            NaN             NaN      NaN       NaN  \n",
       "1        NaN   NaN            NaN             NaN      NaN       NaN  \n",
       "2        NaN   NaN            NaN             NaN      NaN       NaN  \n",
       "3        NaN   NaN            NaN             NaN      NaN       NaN  \n",
       "4        NaN   NaN            NaN             NaN      NaN       NaN  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results = training_run.history(samples=10000)\n",
    "training_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = training_results.groupby(\"epoch\")[\"train/loss\"].mean().plot();\n",
    "training_results[\"validation/loss\"].dropna().plot(logy=True); ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19675205647945404"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 10\n",
    "training_results[\"validation/loss\"].dropna().iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The charts and webpages in Weights & Biases\n",
    "are substantially more useful than ephemeral stdouts or raw logs on disk.\n",
    "\n",
    "If you're spun up on the project,\n",
    "they accelerate debugging, exploration, and discovery.\n",
    "\n",
    "If not, they're not so much useful as they are overwhelming.\n",
    "\n",
    "We need to synthesize the raw logged data into information.\n",
    "This helps us communicate our work with other stakeholders,\n",
    "preserve knowledge and prevent repetition of work,\n",
    "and surface insights faster.\n",
    "\n",
    "These workflows are supported by the W&B Reports feature\n",
    "([docs here](https://docs.wandb.ai/guides/reports)),\n",
    "which mix W&B charts and tables with explanatory markdown text and embeds.\n",
    "\n",
    "Below are some common report patterns and\n",
    "use cases and examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the examples are from the FSDL Text Recognizer project.\n",
    "You can find more of them\n",
    "[here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/-Report-of-Reports---VmlldzoyMjEwNDM5),\n",
    "where we've organized them into a report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dashboards are a structured subset of the output from one or more experiments,\n",
    "designed for quickly surfacing issues or insights,\n",
    "like an accuracy or performance regression\n",
    "or a change in the data distribution.\n",
    "\n",
    "Use cases:\n",
    "- show the basic state of ongoing experiment\n",
    "- compare one experiment to another\n",
    "- select the most important charts so you can spin back up into context on a project more quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Training-Run-2022-06-02--VmlldzoyMTAyOTkw\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda61452410>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dashboard_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Training-Run-2022-06-02--VmlldzoyMTAyOTkw\"\n",
    "\n",
    "IFrame(src=dashboard_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Request Documentation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most software codebases,\n",
    "pull requests are a key focal point\n",
    "for units of work that combine\n",
    "short-term communication and long-term information tracking.\n",
    "\n",
    "In ML codebases, it's more difficult to bring\n",
    "sufficient information together to make PRs as useful.\n",
    "At FSDL, we like to add documentary\n",
    "reports with one or a small number of charts\n",
    "that connect logged information in the experiment management system\n",
    "to state in the version control software.\n",
    "\n",
    "Use cases:\n",
    "- communication of results within a team, e.g. code review\n",
    "- record-keeping that links pull request pages to raw logged info and makes it discoverable\n",
    "- improving confidence in PR correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Overfit-Check-After-Refactor--VmlldzoyMDY5MjI1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda61a47cd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugfix_doc_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Overfit-Check-After-Refactor--VmlldzoyMDY5MjI1\"\n",
    "\n",
    "IFrame(src=bugfix_doc_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog Post Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sufficient effort, the logged data in the experiment management system\n",
    "can be made clear enough to be consumed,\n",
    "sufficiently contextualized to be useful outside the team, and\n",
    "even beautiful.\n",
    "\n",
    "The result is a report that's closer to a blog post than a dashboard or internal document.\n",
    "\n",
    "Use cases:\n",
    "- communication between teams or vertically in large organizations\n",
    "- external technical communication for branding and recruiting\n",
    "- attracting users or contributors\n",
    "\n",
    "Check out this example, from the Craiyon.ai / DALL·E Mini project, by FSDL alumnus\n",
    "[Boris Dayma](https://twitter.com/borisdayma)\n",
    "and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA#training-dall-e-mini\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda602ae590>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dalle_mini_blog_url = \"https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA#training-dall-e-mini\"\n",
    "\n",
    "IFrame(src=dalle_mini_blog_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our choices, like the depth of our network, the nonlinearities of our layers,\n",
    "and the learning rate and other parameters of our optimizer, cannot be\n",
    "([easily](https://arxiv.org/abs/1606.04474))\n",
    "chosen by descent of the gradient of a loss function.\n",
    "\n",
    "But these parameters that impact the values of the parameters\n",
    "we directly optimize with gradients, or _hyperparameters_,\n",
    "can still be optimized,\n",
    "essentially by trying options and selecting the values that worked best.\n",
    "\n",
    "In general, you can attain much of the benefit of hyperparameter optimization with minimal effort.\n",
    "\n",
    "Expending more compute can squeeze small amounts of additional validation or test performance\n",
    "that makes for impressive results on leaderboards but typically doesn't translate\n",
    "into better user experience.\n",
    "\n",
    "In general, the FSDL recommendation is to use the hyperparameter optimization workflows\n",
    "built into your other tooling.\n",
    "\n",
    "Weights & Biases makes the most straightforward forms of hyperparameter optimization trivially easy\n",
    "([docs](https://docs.wandb.ai/guides/sweeps)).\n",
    "\n",
    "It also supports a number of more advanced tools, like\n",
    "[Hyperband](https://docs.wandb.ai/guides/sweeps/configuration#early_terminate)\n",
    "for early termination of poorly-performing runs.\n",
    "\n",
    "We can use the same training script and we don't need to run an optimization server.\n",
    "\n",
    "We just need to write a configuration yaml file\n",
    "([docs](https://docs.wandb.ai/guides/sweeps/configuration)),\n",
    "like the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/simple-overfit-sweep.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/simple-overfit-sweep.yaml\n",
    "# first we specify what we're sweeping\n",
    "# we specify a program to run\n",
    "program: training/run_experiment.py\n",
    "# we optionally specify how to run it, including setting default arguments\n",
    "command:  \n",
    "    - ${env}\n",
    "    - ${interpreter}\n",
    "    - ${program}\n",
    "    - \"--wandb\"\n",
    "    - \"--overfit_batches\"\n",
    "    - \"1\"\n",
    "    - \"--log_every_n_steps\"\n",
    "    - \"25\"\n",
    "    - \"--max_epochs\"\n",
    "    - \"100\"\n",
    "    - \"--limit_test_batches\"\n",
    "    - \"0\"\n",
    "    - ${args}  # these arguments come from the sweep parameters below\n",
    "\n",
    "# and we specify which parameters to sweep over, what we're optimizing, and how we want to optimize it\n",
    "method: random  # generally, random searches perform well, can also be \"grid\" or \"bayes\"\n",
    "metric:\n",
    "    name: train/loss\n",
    "    goal: minimize\n",
    "parameters:  \n",
    "    # LineCNN hyperparameters\n",
    "    window_width:\n",
    "        values: [8, 16, 32, 64]\n",
    "    window_stride:\n",
    "        values: [4, 8, 16, 32]\n",
    "    # Transformer hyperparameters\n",
    "    tf_layers:\n",
    "        values: [1, 2, 4, 8]\n",
    "    # we can also fix some values, just like we set default arguments\n",
    "    gpus:\n",
    "        value: 1\n",
    "    model_class:\n",
    "        value: LineCNNTransformer\n",
    "    data_class:\n",
    "        value: IAMLines\n",
    "    loss:\n",
    "        value: transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the config we launch a \"controller\":\n",
    "a lightweight process that just decides what hyperparameters to try next\n",
    "and coordinates the heavierweight training.\n",
    "\n",
    "This lives on the W&B servers, so there are no headaches about opening ports for communication,\n",
    "cleaning up when it's done, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: training/simple-overfit-sweep.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33mt81ogjf2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/terps/fsdl-line-recognizer-2022/sweeps/t81ogjf2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent terps/fsdl-line-recognizer-2022/t81ogjf2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb sweep training/simple-overfit-sweep.yaml --project fsdl-line-recognizer-2022\n",
    "simple_sweep_id = wb_api.project(\"fsdl-line-recognizer-2022\").sweeps()[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we can launch an \"agent\" to follow the orders of the controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
      "2022-10-03 14:04:25,181 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2022-10-03 14:04:25,648 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2022-10-03 14:04:25,649 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tdata_class: IAMLines\n",
      "\tgpus: 1\n",
      "\tloss: transformer\n",
      "\tmodel_class: LineCNNTransformer\n",
      "\ttf_layers: 8\n",
      "\twindow_stride: 4\n",
      "\twindow_width: 8\n",
      "2022-10-03 14:04:25,652 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python training/run_experiment.py --wandb --overfit_batches 1 --log_every_n_steps 25 --max_epochs 100 --limit_test_batches 0 --data_class=IAMLines --gpus=1 --loss=transformer --model_class=LineCNNTransformer --tf_layers=8 --window_stride=4 --window_width=8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterps\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1mtraining/logs/wandb/run-20221003_140429-16o2dyrt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mlight-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/terps/fsdl-line-recognizer-2022\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/terps/fsdl-line-recognizer-2022/sweeps/t81ogjf2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/terps/fsdl-line-recognizer-2022/runs/16o2dyrt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gpus' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'model_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'window_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'window_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tf_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loss' was locked by 'sweep' (ignored update).\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n",
      "2022-10-03 14:04:30,670 - wandb.wandb_agent - INFO - Running runs: ['16o2dyrt']\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                      | Type               | Params\n",
      "------------------------------------------------------------------\n",
      "0  | model                     | LineCNNTransformer | 6.5 M \n",
      "1  | model.line_cnn            | LineCNN            | 1.2 M \n",
      "2  | model.embedding           | Embedding          | 21.2 K\n",
      "3  | model.fc                  | Linear             | 21.3 K\n",
      "4  | model.pos_encoder         | PositionalEncoding | 0     \n",
      "5  | model.transformer_decoder | TransformerDecoder | 5.3 M \n",
      "6  | train_acc                 | Accuracy           | 0     \n",
      "7  | val_acc                   | Accuracy           | 0     \n",
      "8  | test_acc                  | Accuracy           | 0     \n",
      "9  | val_cer                   | CharacterErrorRate | 0     \n",
      "10 | test_cer                  | CharacterErrorRate | 0     \n",
      "11 | loss_fn                   | CrossEntropyLoss   | 0     \n",
      "------------------------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.906    Total estimated model params size (MB)\n",
      "Model State Dict Disk Size: 25.97 MB\n",
      "/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:472: UserWarning: You requested to overfit but enabled training dataloader shuffling. We are turning off the training dataloader shuffling for you.\n",
      "  \"You requested to overfit but enabled training dataloader shuffling.\"\n",
      "/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1931: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=25). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  category=PossibleUserWarning,\n",
      "Epoch 0:   0%|                                            | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"training/run_experiment.py\", line 174, in <module>\n",
      "    main()\n",
      "  File \"training/run_experiment.py\", line 162, in main\n",
      "    trainer.fit(lit_model, datamodule=data)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 769, in fit\n",
      "    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 721, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 809, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1234, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1321, in _run_stage\n",
      "    return self._run_train()\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1351, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\", line 268, in advance\n",
      "    self._outputs = self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 171, in advance\n",
      "    batch = next(data_fetcher)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/utilities/fetching.py\", line 184, in __next__\n",
      "    return self.fetching_function()\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/utilities/fetching.py\", line 259, in fetching_function\n",
      "    self._fetch_next_batch(self.dataloader_iter)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/utilities/fetching.py\", line 273, in _fetch_next_batch\n",
      "    batch = next(iterator)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/supporters.py\", line 553, in __next__\n",
      "    return self.request_next_batch(self.loader_iters)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/supporters.py\", line 565, in request_next_batch\n",
      "    return apply_to_collection(loader_iters, Iterator, next)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py\", line 99, in apply_to_collection\n",
      "    return function(data, *args, **kwargs)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1347, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1373, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/_utils.py\", line 461, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in pin memory thread for device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 34, in _pin_memory_loop\n",
      "    data = pin_memory(data, device)\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 65, in pin_memory\n",
      "    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 65, in <listcomp>\n",
      "    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n",
      "  File \"/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 50, in pin_memory\n",
      "    return data.pin_memory(device)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:03<?, ?it/s]                                  \n",
      "2022-10-03 14:04:51,077 - wandb.wandb_agent - INFO - Cleaning up finished run: 16o2dyrt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   optimizer/lr-Adam ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        size/mb_disk ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        size/nparams ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   optimizer/lr-Adam 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        size/mb_disk 25.97182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        size/nparams 6476403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlight-sweep-2\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/terps/fsdl-line-recognizer-2022/runs/16o2dyrt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1mtraining/logs/wandb/run-20221003_140429-16o2dyrt/logs\u001b[0m\n",
      "CPU times: user 1.47 s, sys: 633 ms, total: 2.1 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# interrupt twice to terminate this cell if it's running too long,\n",
    "#   it can be over 15 minutes with some hyperparameters\n",
    "\n",
    "!wandb agent --project fsdl-line-recognizer-2022 --entity {wb_api.default_entity} --count=1 {simple_sweep_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell runs only a single experiment, because we provided the `--count` argument with a value of `1`.\n",
    "\n",
    "If not provided, the agent will run forever for random or Bayesian sweeps\n",
    "or until the sweep is terminated, which can be done from the W&B interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agents make for a slick workflow for distributing sweeps across GPUs.\n",
    "\n",
    "We can just change the `CUDA_VISIBLE_DEVICES` environment variable,\n",
    "which controls which GPUs are accessible by a process, to launch\n",
    "parallel agents on separate GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 wandb agent $SWEEP_ID\n",
    "# open another terminal\n",
    "CUDA_VISIBLE_DEVICES=1 wandb agent $SWEEP_ID\n",
    "# and so on\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFx-OhF837Bp"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include optional exercises with the labs for learners who want to dive deeper on specific topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟Contribute to a hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've kicked off a big hyperparameter search on the `LineCNNTransformer` that anyone can join!\n",
    "\n",
    "There are ~10,000,000 potential hyperparameter combinations,\n",
    "and each takes 30 minutes to test,\n",
    "so checking each possiblity will take over 500 years of compute time.\n",
    "Best get cracking then!\n",
    "\n",
    "Run the cell below to pull up a dashboard and print the URL where you can check on the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/fullstackdeeplearning/fsdl-line-recognizer-2022/sweeps/e0eo43eu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/fullstackdeeplearning/fsdl-line-recognizer-2022/sweeps/e0eo43eu\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fda61bed6d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_entity = \"fullstackdeeplearning\"\n",
    "sweep_project = \"fsdl-line-recognizer-2022\"\n",
    "sweep_id = \"e0eo43eu\"\n",
    "sweep_url = f\"https://wandb.ai/{sweep_entity}/{sweep_project}/sweeps/{sweep_id}\"\n",
    "\n",
    "print(sweep_url)\n",
    "IFrame(src=sweep_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve information about the sweep from the API,\n",
    "including the hyperparameters being swept over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_info = wb_api.sweep(\"/\".join([sweep_entity, sweep_project, sweep_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': {'values': [8, 16, 64]},\n",
       " 'conv_dim': {'values': [16, 32, 64, 128]},\n",
       " 'fc_dim': {'values': [32, 128, 512, 1024]},\n",
       " 'fc_dropout': {'values': [0, 0.05, 0.25, 0.5]},\n",
       " 'gpus': {'value': 1},\n",
       " 'loss': {'value': 'transformer'},\n",
       " 'lr': {'values': ['1e-06', '1e-05', 0.0001, 0.001]},\n",
       " 'max_epochs': {'value': -1},\n",
       " 'max_time': {'value': '00:00:30:00'},\n",
       " 'tf_dim': {'values': [4, 16, 64, 256]},\n",
       " 'tf_dropout': {'values': [0, 0.05, 0.25, 0.5]},\n",
       " 'tf_fc_dim': {'values': [4, 16, 64, 256]},\n",
       " 'tf_layers': {'values': [1, 2, 4, 8]},\n",
       " 'tf_nhead': {'values': [2, 4]},\n",
       " 'window_stride': {'values': [4, 8, 16, 32]},\n",
       " 'window_width': {'values': [8, 16, 32, 64]}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = sweep_info.config[\"parameters\"]\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to contribute to this sweep,\n",
    "run the cell below after changing the count to a number greater than 0.\n",
    "\n",
    "Each iteration runs for 30 minutes if it does not crash,\n",
    "e.g. due to out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0  # off by default, increase it to join in!\n",
    "\n",
    "if count:\n",
    "    !wandb agent {sweep_id} --entity {sweep_entity} --project {sweep_project} --count {count}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Write some manual logging in `wandb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FSDL Text Recognizer codebase,\n",
    "we almost exclusively log to W&B through Lightning,\n",
    "rather than through the `wandb` Python SDK.\n",
    "\n",
    "If you're interested in learning how to use W&B directly, e.g. with another training framework,\n",
    "try out this quick exercise that introduces the key players in the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below starts a run with `wandb.init` and provides configuration hyperparameters with `wandb.config`.\n",
    "\n",
    "It also calculates a `loss` value and saves a text file, `logs/hello.txt`.\n",
    "\n",
    "Add W&B metric and artifact logging to this cell:\n",
    "- use [`wandb.log`](https://docs.wandb.ai/guides/track/log) to log the loss on each step\n",
    "- use [`wandb.log_artifact`](https://docs.wandb.ai/guides/artifacts) to save `logs/hello.txt` in an artifact with the name `hello` and whatever type you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/terps/.git/fsdl-text-recognizer-2022-labs/lab04/wandb/run-20221003_144456-3u8xlk0i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/terps/trying-wandb/runs/3u8xlk0i\" target=\"_blank\">fiery-snowball-8</a></strong> to <a href=\"https://wandb.ai/terps/trying-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fiery-snowball-8</strong>: <a href=\"https://wandb.ai/terps/trying-wandb/runs/3u8xlk0i\" target=\"_blank\">https://wandb.ai/terps/trying-wandb/runs/3u8xlk0i</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221003_144456-3u8xlk0i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "project = \"trying-wandb\"\n",
    "config = {\"steps\": 50}\n",
    "\n",
    "\n",
    "with wandb.init(project=project, config=config) as run:\n",
    "    steps = wandb.config[\"steps\"]\n",
    "    \n",
    "    for ii in range(steps):\n",
    "        loss = math.exp(-ii) + random.random() / (ii + 1)  # ML means making the loss go down\n",
    "        wandb.log({'loss': loss})\n",
    "        \n",
    "    with open(\"logs/hello.txt\", \"w\") as f:\n",
    "        f.write(\"hello from wandb, my dudes!\")\n",
    "    \n",
    "    artifact = wandb.Artifact('hello', type='dataset')\n",
    "    run.log_artifact(artifact)\n",
    "        \n",
    "        \n",
    "    run_id = run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've correctly completed the exercise, the cell below will print only 🥞 emojis and no 🥲s before opening the run in an iframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss logged successfully 🥞\n",
      "loss logged on all steps 🥞\n",
      "hello artifact not logged 🥲\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/terps/trying-wandb/runs/3u8xlk0i?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<Run terps/trying-wandb/3u8xlk0i (finished)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_run = wb_api.run(f\"{project}/{run_id}\")\n",
    "\n",
    "# check for logged loss data\n",
    "if \"loss\" not in hello_run.history().keys():\n",
    "    print(\"loss not logged 🥲\")\n",
    "else:\n",
    "    print(\"loss logged successfully 🥞\")\n",
    "    if len(hello_run.history()[\"loss\"]) != steps:\n",
    "        print(\"loss not logged on all steps 🥲\")\n",
    "    else:\n",
    "        print(\"loss logged on all steps 🥞\")\n",
    "\n",
    "artifacts =  hello_run.logged_artifacts()\n",
    "\n",
    "# check for artifact with the right name\n",
    "if \"hello:v0\" not in [artifact.name for artifact in artifacts]:\n",
    "    print(\"hello artifact not logged 🥲\")\n",
    "else:\n",
    "    print(\"hello artifact logged successfully 🥞\")\n",
    "    # check for the file inside the artifacts\n",
    "    if \"hello.txt\" not in sum([list(artifact.manifest.entries.keys()) for artifact in artifacts], []):\n",
    "        print(\"could not find hello.txt 🥲\")\n",
    "    else:\n",
    "        print(\"hello.txt logged successfully 🥞\")\n",
    "    \n",
    "    \n",
    "hello_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Find good hyperparameters for the `LineCNNTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default hyperparameters for the `LineCNNTransformer` are not particularly carefully tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and find some better hyperparameters: choices that achieve a lower loss on the full dataset faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe interesting phenomena during training,\n",
    "from promising hyperparameter combos to software bugs to strange model behavior,\n",
    "turn the charts into a W&B report and share it with the FSDL community or\n",
    "[open an issue on GitHub](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022/issues)\n",
    "with a link to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_experiment.py [--logger [LOGGER]]\n",
      "                         [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                         [--enable_checkpointing [ENABLE_CHECKPOINTING]]\n",
      "                         [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                         [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                         [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                         [--process_position PROCESS_POSITION]\n",
      "                         [--num_nodes NUM_NODES]\n",
      "                         [--num_processes NUM_PROCESSES] [--devices DEVICES]\n",
      "                         [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                         [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                         [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                         [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                         [--enable_progress_bar [ENABLE_PROGRESS_BAR]]\n",
      "                         [--overfit_batches OVERFIT_BATCHES]\n",
      "                         [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                         [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                         [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                         [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                         [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                         [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
      "                         [--max_time MAX_TIME]\n",
      "                         [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                         [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                         [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                         [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                         [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                         [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                         [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                         [--accelerator ACCELERATOR] [--strategy STRATEGY]\n",
      "                         [--sync_batchnorm [SYNC_BATCHNORM]]\n",
      "                         [--precision PRECISION]\n",
      "                         [--enable_model_summary [ENABLE_MODEL_SUMMARY]]\n",
      "                         [--weights_summary WEIGHTS_SUMMARY]\n",
      "                         [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                         [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                         [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                         [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                         [--deterministic [DETERMINISTIC]]\n",
      "                         [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                         [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                         [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                         [--detect_anomaly [DETECT_ANOMALY]]\n",
      "                         [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                         [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                         [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                         [--amp_level AMP_LEVEL]\n",
      "                         [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                         [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                         [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "                         [--terminate_on_nan [TERMINATE_ON_NAN]] [--wandb]\n",
      "                         [--data_class DATA_CLASS] [--model_class MODEL_CLASS]\n",
      "                         [--load_checkpoint LOAD_CHECKPOINT]\n",
      "                         [--stop_early STOP_EARLY] [--batch_size BATCH_SIZE]\n",
      "                         [--num_workers NUM_WORKERS]\n",
      "                         [--augment_data AUGMENT_DATA] [--conv_dim CONV_DIM]\n",
      "                         [--fc_dim FC_DIM] [--fc_dropout FC_DROPOUT]\n",
      "                         [--window_width WINDOW_WIDTH]\n",
      "                         [--window_stride WINDOW_STRIDE]\n",
      "                         [--limit_output_length] [--tf_dim TF_DIM]\n",
      "                         [--tf_fc_dim TF_FC_DIM] [--tf_dropout TF_DROPOUT]\n",
      "                         [--tf_layers TF_LAYERS] [--tf_nhead TF_NHEAD]\n",
      "                         [--optimizer OPTIMIZER] [--lr LR]\n",
      "                         [--one_cycle_max_lr ONE_CYCLE_MAX_LR]\n",
      "                         [--one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS]\n",
      "                         [--loss LOSS] [--help]\n",
      "\n",
      "optional arguments:\n",
      "  --wandb               If passed, logs experiment results to Weights &\n",
      "                        Biases. Otherwise logs only to local Tensorboard.\n",
      "  --data_class DATA_CLASS\n",
      "                        String identifier for the data class, relative to\n",
      "                        text_recognizer.data.\n",
      "  --model_class MODEL_CLASS\n",
      "                        String identifier for the model class, relative to\n",
      "                        text_recognizer.models.\n",
      "  --load_checkpoint LOAD_CHECKPOINT\n",
      "                        If passed, loads a model from the provided path.\n",
      "  --stop_early STOP_EARLY\n",
      "                        If non-zero, applies early stopping, with the provided\n",
      "                        value as the 'patience' argument. Default is 0.\n",
      "  --help, -h\n",
      "\n",
      "pl.Trainer:\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
      "                        If multiple loggers are provided and the `save_dir`\n",
      "                        property of that logger is not set, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
      "                        any of the individual loggers. Default: ``True``.\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
      "                        If ``True``, enable checkpointing. Default: ``None``.\n",
      "                        .. deprecated:: v1.5 ``checkpoint_callback`` has been\n",
      "                        deprecated in v1.5 and will be removed in v1.7. Please\n",
      "                        consider using ``enable_checkpointing`` instead.\n",
      "  --enable_checkpointing [ENABLE_CHECKPOINTING]\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
      "                        ng.trainer.trainer.Trainer.callbacks`. Default:\n",
      "                        ``True``.\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/'\n",
      "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        The value at which to clip gradients. Passing\n",
      "                        ``gradient_clip_val=None`` disables gradient clipping.\n",
      "                        If using Automatic Mixed Precision (AMP), the\n",
      "                        gradients will be unscaled before. Default: ``None``.\n",
      "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        The gradient clipping algorithm to use. Pass\n",
      "                        ``gradient_clip_algorithm=\"value\"`` to clip by value,\n",
      "                        and ``gradient_clip_algorithm=\"norm\"`` to clip by\n",
      "                        norm. By default it will be set to ``\"norm\"``.\n",
      "  --process_position PROCESS_POSITION\n",
      "                        Orders the progress bar when running multiple models\n",
      "                        on same machine. .. deprecated:: v1.5\n",
      "                        ``process_position`` has been deprecated in v1.5 and\n",
      "                        will be removed in v1.7. Please pass :class:`~pytorch_\n",
      "                        lightning.callbacks.progress.TQDMProgressBar` with\n",
      "                        ``process_position`` directly to the Trainer's\n",
      "                        ``callbacks`` argument instead.\n",
      "  --num_nodes NUM_NODES\n",
      "                        Number of GPU nodes for distributed training. Default:\n",
      "                        ``1``.\n",
      "  --num_processes NUM_PROCESSES\n",
      "                        Number of processes for distributed training with\n",
      "                        ``accelerator=\"cpu\"``. Default: ``1``.\n",
      "  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,\n",
      "                        `num_processes` or `ipus`, based on the accelerator\n",
      "                        type.\n",
      "  --gpus GPUS           Number of GPUs to train on (int) or which GPUs to\n",
      "                        train on (list or str) applied per node Default:\n",
      "                        ``None``.\n",
      "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
      "                        If enabled and ``gpus`` or ``devices`` is an integer,\n",
      "                        pick available gpus automatically. This is especially\n",
      "                        useful when GPUs are configured to be in \"exclusive\n",
      "                        mode\", such that only one process at a time can access\n",
      "                        them. Default: ``False``.\n",
      "  --tpu_cores TPU_CORES\n",
      "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
      "                        to train on (1) Default: ``None``.\n",
      "  --ipus IPUS           How many IPUs to train on. Default: ``None``.\n",
      "  --log_gpu_memory LOG_GPU_MEMORY\n",
      "                        None, 'min_max', 'all'. Might slow performance. ..\n",
      "                        deprecated:: v1.5 Deprecated in v1.5.0 and will be\n",
      "                        removed in v1.7.0 Please use the\n",
      "                        ``DeviceStatsMonitor`` callback directly instead.\n",
      "  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\n",
      "                        How often to refresh progress bar (in steps). Value\n",
      "                        ``0`` disables progress bar. Ignored when a custom\n",
      "                        progress bar is passed to\n",
      "                        :paramref:`~Trainer.callbacks`. Default: None, means a\n",
      "                        suitable value will be chosen based on the environment\n",
      "                        (terminal, Google COLAB, etc.). .. deprecated:: v1.5\n",
      "                        ``progress_bar_refresh_rate`` has been deprecated in\n",
      "                        v1.5 and will be removed in v1.7. Please pass :class:`\n",
      "                        ~pytorch_lightning.callbacks.progress.TQDMProgressBar`\n",
      "                        with ``refresh_rate`` directly to the Trainer's\n",
      "                        ``callbacks`` argument instead. To disable the\n",
      "                        progress bar, pass ``enable_progress_bar = False`` to\n",
      "                        the Trainer.\n",
      "  --enable_progress_bar [ENABLE_PROGRESS_BAR]\n",
      "                        Whether to enable to progress bar by default. Default:\n",
      "                        ``False``.\n",
      "  --overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training data (float) or a set\n",
      "                        number of batches (int). Default: ``0.0``.\n",
      "  --track_grad_norm TRACK_GRAD_NORM\n",
      "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
      "                        set to 'inf' infinity-norm. If using Automatic Mixed\n",
      "                        Precision (AMP), the gradients will be unscaled before\n",
      "                        logging them. Default: ``-1``.\n",
      "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Check val every n train epochs. Default: ``1``.\n",
      "  --fast_dev_run [FAST_DEV_RUN]\n",
      "                        Runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test). Default: ``False``.\n",
      "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates grads every k batches or as set up in the\n",
      "                        dict. Default: ``None``.\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to ``max_epochs\n",
      "                        = 1000``. To enable infinite training, set\n",
      "                        ``max_epochs = -1``.\n",
      "  --min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None).\n",
      "  --max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (-1). If ``max_steps = -1`` and ``max_epochs =\n",
      "                        None``, will default to ``max_epochs = 1000``. To\n",
      "                        enable infinite training, set ``max_epochs`` to\n",
      "                        ``-1``.\n",
      "  --min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (``None``).\n",
      "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
      "                        Disabled by default (``None``). The time duration can\n",
      "                        be specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`.\n",
      "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches). Default: ``1.0``.\n",
      "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Pass a\n",
      "                        ``float`` in the range [0.0, 1.0] to check after a\n",
      "                        fraction of the training epoch. Pass an ``int`` to\n",
      "                        check after a fixed number of training batches.\n",
      "                        Default: ``1.0``.\n",
      "  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS\n",
      "                        How often to flush logs to disk (defaults to every 100\n",
      "                        steps). .. deprecated:: v1.5\n",
      "                        ``flush_logs_every_n_steps`` has been deprecated in\n",
      "                        v1.5 and will be removed in v1.7. Please configure\n",
      "                        flushing directly in the logger instead.\n",
      "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps. Default: ``50``.\n",
      "  --accelerator ACCELERATOR\n",
      "                        Supports passing different accelerator types (\"cpu\",\n",
      "                        \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\") as well as custom\n",
      "                        accelerator instances. .. deprecated:: v1.5 Passing\n",
      "                        training strategies (e.g., 'ddp') to ``accelerator``\n",
      "                        has been deprecated in v1.5.0 and will be removed in\n",
      "                        v1.7.0. Please use the ``strategy`` argument instead.\n",
      "  --strategy STRATEGY   Supports different training strategies with aliases as\n",
      "                        well custom strategies. Default: ``None``.\n",
      "  --sync_batchnorm [SYNC_BATCHNORM]\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world. Default: ``False``.\n",
      "  --precision PRECISION\n",
      "                        Double precision (64), full precision (32), half\n",
      "                        precision (16) or bfloat16 precision (bf16). Can be\n",
      "                        used on CPU, GPU, TPUs, HPUs or IPUs. Default: ``32``.\n",
      "  --enable_model_summary [ENABLE_MODEL_SUMMARY]\n",
      "                        Whether to enable model summarization by default.\n",
      "                        Default: ``True``.\n",
      "  --weights_summary WEIGHTS_SUMMARY\n",
      "                        Prints a summary of the weights when training begins.\n",
      "                        .. deprecated:: v1.5 ``weights_summary`` has been\n",
      "                        deprecated in v1.5 and will be removed in v1.7. To\n",
      "                        disable the summary, pass ``enable_model_summary =\n",
      "                        False`` to the Trainer. To customize the summary, pass\n",
      "                        :class:`~pytorch_lightning.callbacks.model_summary.Mod\n",
      "                        elSummary` directly to the Trainer's ``callbacks``\n",
      "                        argument.\n",
      "  --weights_save_path WEIGHTS_SAVE_PATH\n",
      "                        Where to save weights if specified. Will override\n",
      "                        default_root_dir for checkpoints only. Use this if for\n",
      "                        whatever reason you need the checkpoints stored in a\n",
      "                        different place than the logs written in\n",
      "                        `default_root_dir`. Can be remote file paths such as\n",
      "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
      "                        `default_root_dir`. .. deprecated:: v1.6\n",
      "                        ``weights_save_path`` has been deprecated in v1.6 and\n",
      "                        will be removed in v1.8. Please pass ``dirpath``\n",
      "                        directly to the :class:`~pytorch_lightning.callbacks.m\n",
      "                        odel_checkpoint.ModelCheckpoint` callback.\n",
      "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders. Default: ``2``.\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. If there is no checkpoint file at the path,\n",
      "                        an exception is raised. If resuming from mid-epoch\n",
      "                        checkpoint, training will start from the beginning of\n",
      "                        the next epoch. .. deprecated:: v1.5\n",
      "                        ``resume_from_checkpoint`` is deprecated in v1.5 and\n",
      "                        will be removed in v2.0. Please pass the path to\n",
      "                        ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
      "  --profiler PROFILER   To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks. Default: ``None``.\n",
      "  --benchmark [BENCHMARK]\n",
      "                        Sets ``torch.backends.cudnn.benchmark``. Defaults to\n",
      "                        ``True`` if :paramref:`~pytorch_lightning.trainer.trai\n",
      "                        ner.Trainer.deterministic` is ``False``. Overwrite to\n",
      "                        manually set a different value. Default: ``None``.\n",
      "  --deterministic [DETERMINISTIC]\n",
      "                        If ``True``, sets whether PyTorch operations must use\n",
      "                        deterministic algorithms. Default: ``False``.\n",
      "  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a non-negative integer to reload dataloaders\n",
      "                        every n epochs. Default: ``0``.\n",
      "  --auto_lr_find [AUTO_LR_FIND]\n",
      "                        If set to True, will make trainer.tune() run a\n",
      "                        learning rate finder, trying to optimize initial\n",
      "                        learning for faster convergence. trainer.tune() method\n",
      "                        will set the suggested learning rate in self.lr or\n",
      "                        self.learning_rate in the LightningModule. To use a\n",
      "                        different key set a string instead of True with the\n",
      "                        key name. Default: ``False``.\n",
      "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
      "                        Explicitly enables or disables sampler replacement. If\n",
      "                        not specified this will toggled automatically when DDP\n",
      "                        is used. By default it will add ``shuffle=True`` for\n",
      "                        train sampler and ``shuffle=False`` for val/test\n",
      "                        sampler. If you want to customize it, you can set\n",
      "                        ``replace_sampler_ddp=False`` and add your own\n",
      "                        distributed sampler.\n",
      "  --detect_anomaly [DETECT_ANOMALY]\n",
      "                        Enable anomaly detection for the autograd engine.\n",
      "                        Default: ``False``.\n",
      "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
      "                        If set to True, will `initially` run a batch size\n",
      "                        finder trying to find the largest batch size that fits\n",
      "                        into memory. The result will be stored in\n",
      "                        self.batch_size in the LightningModule. Additionally,\n",
      "                        can be set to either `power` that estimates the batch\n",
      "                        size through a power search or `binsearch` that\n",
      "                        estimates the batch size through a binary search.\n",
      "                        Default: ``False``.\n",
      "  --prepare_data_per_node [PREPARE_DATA_PER_NODE]\n",
      "                        If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare\n",
      "                        data .. deprecated:: v1.5 Deprecated in v1.5.0 and\n",
      "                        will be removed in v1.7.0 Please set\n",
      "                        ``prepare_data_per_node`` in ``LightningDataModule``\n",
      "                        and/or ``LightningModule`` directly instead.\n",
      "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins. Default:\n",
      "                        ``None``.\n",
      "  --amp_backend AMP_BACKEND\n",
      "                        The mixed precision backend to use (\"native\" or\n",
      "                        \"apex\"). Default: ``'native''``.\n",
      "  --amp_level AMP_LEVEL\n",
      "                        The optimization level to use (O1, O2, etc...). By\n",
      "                        default it will be set to \"O2\" if ``amp_backend`` is\n",
      "                        set to \"apex\".\n",
      "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
      "                        Whether to force internal logged metrics to be moved\n",
      "                        to cpu. This can save some gpu memory, but can make\n",
      "                        training slower. Use with attention. Default:\n",
      "                        ``False``.\n",
      "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
      "                        How to loop over the datasets when there are multiple\n",
      "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
      "                        ends one epoch when the largest dataset is traversed,\n",
      "                        and smaller datasets reload when running out of their\n",
      "                        data. In 'min_size' mode, all the datasets reload when\n",
      "                        reaching the minimum length of datasets. Default:\n",
      "                        ``\"max_size_cycle\"``.\n",
      "  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]\n",
      "                        Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                        <https://pytorch.org/blog/pytorch-1.6-now-includes-\n",
      "                        stochastic-weight-averaging/>`_. Default: ``False``.\n",
      "                        .. deprecated:: v1.5 ``stochastic_weight_avg`` has\n",
      "                        been deprecated in v1.5 and will be removed in v1.7.\n",
      "                        Please pass :class:`~pytorch_lightning.callbacks.stoch\n",
      "                        astic_weight_avg.StochasticWeightAveraging` directly\n",
      "                        to the Trainer's ``callbacks`` argument instead.\n",
      "  --terminate_on_nan [TERMINATE_ON_NAN]\n",
      "                        If set to True, will terminate training (by raising a\n",
      "                        `ValueError`) at the end of each training batch, if\n",
      "                        any of the parameters or the loss are NaN or +/-inf.\n",
      "                        .. deprecated:: v1.5 Trainer argument\n",
      "                        ``terminate_on_nan`` was deprecated in v1.5 and will\n",
      "                        be removed in 1.7. Please use ``detect_anomaly``\n",
      "                        instead.\n",
      "\n",
      "Data Args:\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of examples to operate on per forward step.\n",
      "                        Default is 128.\n",
      "  --num_workers NUM_WORKERS\n",
      "                        Number of additional processes to load data. Default\n",
      "                        is 12.\n",
      "  --augment_data AUGMENT_DATA\n",
      "\n",
      "Model Args:\n",
      "  --conv_dim CONV_DIM\n",
      "  --fc_dim FC_DIM\n",
      "  --fc_dropout FC_DROPOUT\n",
      "  --window_width WINDOW_WIDTH\n",
      "                        Width of the window that will slide over the input\n",
      "                        image.\n",
      "  --window_stride WINDOW_STRIDE\n",
      "                        Stride of the window that will slide over the input\n",
      "                        image.\n",
      "  --limit_output_length\n",
      "  --tf_dim TF_DIM\n",
      "  --tf_fc_dim TF_FC_DIM\n",
      "  --tf_dropout TF_DROPOUT\n",
      "  --tf_layers TF_LAYERS\n",
      "  --tf_nhead TF_NHEAD\n",
      "\n",
      "LitModel Args:\n",
      "  --optimizer OPTIMIZER\n",
      "                        optimizer class from torch.optim\n",
      "  --lr LR\n",
      "  --one_cycle_max_lr ONE_CYCLE_MAX_LR\n",
      "  --one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS\n",
      "  --loss LOSS           loss function from torch.nn.functional\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the sweep_info.config above to see the model and data hyperparameters\n",
    "#   read through the --help output for all potential arguments\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 5 \\\n",
    "  --log_every_n_steps 50 --wandb --limit_test_batches 0.1 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 \\\n",
    "  --help  # remove this line to run an experiment instead of printing help\n",
    "    \n",
    "last_hyperparam_expt = wandb.run  # in case you want to pull URLs, look up in API, etc., as in code above\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟🌟🌟 Add logging of tensor statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to logging model inputs and outputs as human-interpretable media,\n",
    "it's also frequently useful to see information about their numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in learning more about metric calculation and logging with Lightning,\n",
    "use [`torchmetrics`](https://torchmetrics.readthedocs.io/en/v0.7.3/)\n",
    "to add tensor statistic logging to the `LineCNNTransformer`.\n",
    "\n",
    "`torchmetrics` comes with built in statistical metrics, like `MinMetric`, `MaxMetric`, and `MeanMetric`.\n",
    "\n",
    "All three are useful, but start by adding just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your metric with `training/run_experiment.py`, you'll need to open and edit the `text_recognizer/lit_model/base.py` and `text_recognizer/lit_model/transformer.py` files\n",
    "- Add the metrics to the `BaseImageToTextLitModel`'s `__init__` method, around where `CharacterErrorRate` appears.\n",
    "  - You'll also need to decide whether to calculate separate train/validation/test versions. Whatever you do, start by implementing just one.\n",
    "- In the appropriate `_step` methods of the `TransformerLitModel`, add metric calculation and logging for `Min`, `Max`, and/or `Mean`.\n",
    "  - Base your code on the calculation and logging of the `val_cer` metric.\n",
    "  - `sync_dist=True` is only important in distributed training settings, so you might not notice any issues regardless of that argument's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an extra challenge, use `MeanSquaredError` to implement a `VarianceMetric`. _Hint_: one way is to use `torch.zeros_like` and `torch.mean`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMKpeodqRUzgu0VjkCVMBeJ",
   "collapsed_sections": [],
   "name": "lab04_experiments.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "fsdl",
   "language": "python",
   "name": "fsdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
