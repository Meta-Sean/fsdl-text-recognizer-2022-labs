{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 02a: PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- The core components of a PyTorch Lightning training loop: `LightningModule`s and `Trainer`s.\n",
    "- Useful quality-of-life improvements offered by PyTorch Lightning: `LightningDataModule`s, `Callback`s, and `Metric`s\n",
    "- How we use these features in the FSDL codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=.:\n",
      ".:\n",
      "/home/terps/.git/fsdl-text-recognizer-2022-labs/lab02\n",
      "\u001b[0m\u001b[01;34mlightning_logs\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34mtext_recognizer\u001b[0m/  \u001b[01;34mtraining\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "lab_idx = 2\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZN4bGgsgWc_"
   },
   "source": [
    "# Why Lightning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP8iJW_bg7IC"
   },
   "source": [
    "PyTorch is a powerful library for executing differentiable\n",
    "tensor operations with hardware acceleration\n",
    "and it includes many neural network primitives,\n",
    "but it has no concept of \"training\".\n",
    "At a high level, an `nn.Module` is a stateful function with gradients\n",
    "and a `torch.optim.Optimizer` can update that state using gradients,\n",
    "but there's no pre-built tools in PyTorch to iteratively generate those gradients from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7gIA-Efy91E"
   },
   "source": [
    "So the first thing many folks do in PyTorch is write that code --\n",
    "a \"training loop\" to iterate over their `DataLoader`,\n",
    "which in pseudocode might look something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3ewkWrwzDA8"
   },
   "source": [
    "```python\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = some_loss_function(inputs, outputs)\n",
    "    \n",
    "    optimizer.zero_gradients()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYUtiJWize82"
   },
   "source": [
    "This is a solid start, but other needs immediately arise.\n",
    "You'll want to run your model on validation and test data,\n",
    "which need their own `DataLoader`s.\n",
    "Once finished, you'll want to save your model --\n",
    "and for long-running jobs, you probably want\n",
    "to save checkpoints of the training process\n",
    "so that it can be resumed in case of a crash.\n",
    "For state-of-the-art model performance in many domains,\n",
    "you'll want to distribute your training across multiple nodes/machines\n",
    "and across multiple GPUs within those nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0untumvjy5fm"
   },
   "source": [
    "That's just the tip of the iceberg, and you want\n",
    "all those features to work for lots of models and datasets,\n",
    "not just the one you're writing now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNPpi4OZjMbu"
   },
   "source": [
    "You don't want to write all of this yourself.\n",
    "\n",
    "So unless you are at a large organization that has a dedicated team\n",
    "for building that \"framework\" code,\n",
    "you'll want to use an existing library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnQuyVqUjJy8"
   },
   "source": [
    "PyTorch Lightning is a popular framework on top of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ecipNFTgZDt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pytorch-lightning.readthedocs.io/en/1.6.3/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "version = pl.__version__\n",
    "\n",
    "docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/\"  # version can also be latest, stable\n",
    "docs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bE82xoEikWkh"
   },
   "source": [
    "At its core, PyTorch Lightning provides\n",
    "\n",
    "1. the `pl.Trainer` class, which organizes and executes your training, validation, and test loops, and\n",
    "2. the `pl.LightningModule` class, which links optimizers to models and defines how the model behaves during training, validation, and testing.\n",
    "\n",
    "Both of these are kitted out with all the features\n",
    "a cutting-edge deep learning codebase needs:\n",
    "- flags for switching device types and distributed computing strategy\n",
    "- saving, checkpointing, and resumption\n",
    "- calculation and logging of metrics\n",
    "\n",
    "and much more.\n",
    "\n",
    "Importantly these features can be easily\n",
    "added, removed, extended, or bypassed\n",
    "as desired, meaning your code isn't constrained by the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuJUDmCeT3RK"
   },
   "source": [
    "In some ways, you can think of Lightning as a tool for \"organizing\" your PyTorch code,\n",
    "as shown in the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wTt0TBs5TZpm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"720\"\n",
       "            src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f29b2af2510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "\n",
    "display.IFrame(src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\",\n",
    "               width=720, height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGwpDn5GWn_X"
   },
   "source": [
    "That's opposed to the other way frameworks are designed,\n",
    "to provide abstractions over the lower-level library\n",
    "(here, PyTorch).\n",
    "\n",
    "Because of this \"organize don't abstract\" style,\n",
    "writing PyTorch Lightning code involves\n",
    "a lot of over-riding of methods --\n",
    "you inherit from a class\n",
    "and then implement the specific version of a general method\n",
    "that you need for your code,\n",
    "rather than Lightning providing a bunch of already\n",
    "fully-defined classes that you just instantiate,\n",
    "using arguments for configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXiUcQwan39S"
   },
   "source": [
    "# The `pl.LightningModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3FffD5Vn6we"
   },
   "source": [
    "The first of our two core classes,\n",
    "the `LightningModule`,\n",
    "is like a souped-up `torch.nn.Module` --\n",
    "it inherits all of the `Module` features,\n",
    "but adds more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0QWwSStJTP28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "issubclass(pl.LightningModule, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1wiBVSTuHNT"
   },
   "source": [
    "To demonstrate how this class works,\n",
    "we'll build up a `LinearRegression` model dynamically,\n",
    "method by method.\n",
    "\n",
    "For this example we hard code lots of the details,\n",
    "but the real benefit comes when the details are configurable.\n",
    "\n",
    "In order to have a realistic example as well,\n",
    "we'll compare to the actual code\n",
    "in the `BaseLitModel` we use in the codebase\n",
    "as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fPARncfQ3ohz"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models import BaseLitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseLitModel??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myyL0vYU3z0a"
   },
   "source": [
    "A `pl.LightningModule` is a `torch.nn.Module`,\n",
    "so the basic definition looks the same:\n",
    "we need `__init__` and `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-c0ylFO9rW_t"
   },
   "outputs": [],
   "source": [
    "class LinearRegression(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
    "\n",
    "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
    "        self.model = torch.nn.Linear(in_features=1, out_features=1)\n",
    "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
    "\n",
    "    # optionally, define a forward method\n",
    "    def forward(self, xs):\n",
    "        return self.model(xs)  # we like to just call the model's forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY1yoGTy6CBu"
   },
   "source": [
    "But just the minimal definition for a `torch.nn.Module` isn't sufficient\n",
    "\n",
    "If we try to use the class above with the `Trainer`, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tBWh_uHu5rmU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "\tNo `training_step()` method defined. Lightning `Trainer` expects as minimum a\n",
      "\t`training_step()`, `train_dataloader()` and `configure_optimizers()` to be\n",
      "\tdefined.\n"
     ]
    }
   ],
   "source": [
    "import logging  # import some stdlib components to control what's display\n",
    "import textwrap\n",
    "import traceback\n",
    "\n",
    "\n",
    "try:  # try using the LinearRegression LightningModule defined above\n",
    "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)  # hide some info for now\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # we'll explain how the Trainer works in a bit\n",
    "    trainer = pl.Trainer(gpus=int(torch.cuda.is_available()), max_epochs=1)\n",
    "    trainer.fit(model=model)  \n",
    "\n",
    "except pl.utilities.exceptions.MisconfigurationException as error:\n",
    "    print(\"Error:\", *textwrap.wrap(str(error), 80), sep=\"\\n\\t\")  # show the error without raising it\n",
    "\n",
    "finally:  # bring back info-level logging\n",
    "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5ni7xe5CgUt"
   },
   "source": [
    "The error message says we need some more methods.\n",
    "\n",
    "Two of them are mandatory components of the `LightningModule`: `.training_step` and `.configure_optimizers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37BXP7nAoBik"
   },
   "source": [
    "#### `.training_step`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah9MjWz2plFv"
   },
   "source": [
    "The `training_step` method defines,\n",
    "naturally enough,\n",
    "what to do during a single step of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plWEvWG_zRia"
   },
   "source": [
    "Roughly, it gets used like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RbxZ4idy-C5"
   },
   "source": [
    "```python\n",
    "\n",
    "# pseudocode modified from the Lightning documentation\n",
    "\n",
    "# put model in train mode\n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    # run the train step\n",
    "    loss = training_step(batch)\n",
    "\n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cemh_hGJ53nL"
   },
   "source": [
    "Effectively, it maps a batch to a loss value,\n",
    "so that PyTorch can backprop through that loss.\n",
    "\n",
    "The `.training_step` for our `LinearRegression` model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X8qW2VRRsPI2"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def training_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "    xs, ys = batch  # unpack the batch\n",
    "    outs = self(xs)  # apply the model\n",
    "    loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "LinearRegression.training_step = training_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2e8m3BRCIx6"
   },
   "source": [
    "If you've written PyTorch code before, you'll notice that we don't mention devices\n",
    "or other tensor metadata here -- that's handled for us by Lightning, which is a huge relief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkvNpfwqpns5"
   },
   "source": [
    "You can additionally define\n",
    "a `validation_step` and a `test_step`\n",
    "to define the model's behavior during\n",
    "validation and testing loops.\n",
    "\n",
    "You're invited to define these steps\n",
    "in the exercises at the end of the lab.\n",
    "\n",
    "Inside this step is also where you might calculate other\n",
    "values related to inputs, outputs, and loss,\n",
    "like non-differentiable metrics (e.g. accuracy, precision, recall).\n",
    "\n",
    "So our `BaseLitModel`'s got a slightly more complex `training_step` method,\n",
    "and the details of the forward pass are deferred to `._run_on_batch` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xpBkRczao1hr"
   },
   "outputs": [],
   "source": [
    "BaseLitModel.training_step??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guhoYf_NoEyc"
   },
   "source": [
    "#### `.configure_optimizers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCIAWoCEtIU7"
   },
   "source": [
    "Thanks to `training_step` we've got a loss, and PyTorch can turn that into a gradient.\n",
    "\n",
    "But we need more than a gradient to do an update.\n",
    "\n",
    "We need an _optimizer_ that can make use of the gradients to update the parameters. In complex cases, we might need more than one optimizer (e.g. GANs).\n",
    "\n",
    "Our second required method, `.configure_optimizers`,\n",
    "sets up the `torch.optim.Optimizer`s \n",
    "(e.g. setting their hyperparameters\n",
    "and pointing them at the `Module`'s parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlnRdIPzvDF"
   },
   "source": [
    "In psuedo-code (modified from the Lightning documentation), it gets used something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WBnfJzszi49"
   },
   "source": [
    "```python\n",
    "optimizer = model.configure_optimizers()\n",
    "\n",
    "for batch_idx, batch in enumerate(data):\n",
    "\n",
    "    def closure():  # wrap the loss calculation\n",
    "        loss = model.training_step(batch, batch_idx, ...)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # optimizer can call the loss calculation as many times as it likes\n",
    "    optimizer.step(closure)  # some optimizers need this, like (L)-BFGS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGsP3DBy7YzW"
   },
   "source": [
    "For our `LinearRegression` model,\n",
    "we just need to instantiate an optimizer and point it at the parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZWrWGgdVt21h"
   },
   "outputs": [],
   "source": [
    "def configure_optimizers(self: LinearRegression) -> torch.optim.Optimizer:\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)  # https://fsdl.me/ol-reliable-img\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "LinearRegression.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta2hs0OLwbtF"
   },
   "source": [
    "You can read more about optimization in Lightning,\n",
    "including how to manually control optimization\n",
    "instead of relying on default behavior,\n",
    "in the docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KXINqlAgwfKy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pytorch-lightning.readthedocs.io/en/1.6.3/common/optimization.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/optimization.html\"\n",
    "optimization_docs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWdKdZDfxmb2"
   },
   "source": [
    "The `configure_optimizers` method for the `BaseLitModel`\n",
    "isn't that much more complex.\n",
    "\n",
    "We just add support for learning rate schedulers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kyRbz0bEpWwd"
   },
   "outputs": [],
   "source": [
    "BaseLitModel.configure_optimizers??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilQCfn7Nm_QP"
   },
   "source": [
    "# The `pl.Trainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RScc0ef97qlc"
   },
   "source": [
    "The `LightningModule` has already helped us organize our code,\n",
    "but it's not really useful until we combine it with the `Trainer`,\n",
    "which relies on the `LightningModule` interface to execute training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBdikPBF86Qp"
   },
   "source": [
    "The `Trainer` is where we make choices like how long to train\n",
    "(`max_epochs`, `min_epochs`, `max_time`, `max_steps`),\n",
    "what kind of acceleration (e.g. `gpus`) or distribution strategy to use,\n",
    "and other settings that might differ across training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YQ4KSdFP3E4Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20, gpus=int(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2l3rGZK7-PL"
   },
   "source": [
    "Before we can actually use the `Trainer`, though,\n",
    "we also need a `torch.utils.data.DataLoader` --\n",
    "nothing new from PyTorch Lightning here,\n",
    "just vanilla PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OcUSD2jP4Ffo"
   },
   "outputs": [],
   "source": [
    "class CorrelatedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, N=10_000):\n",
    "        self.N = N\n",
    "        self.xs = torch.randn(size=(N, 1))\n",
    "        self.ys = torch.randn_like(self.xs) + self.xs  # correlated target data: y ~ N(x, 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.xs[idx], self.ys[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "\n",
    "dataset = CorrelatedDataset()\n",
    "tdl = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0u41JtA8qGo"
   },
   "source": [
    "We can fetch some sample data from the `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z1j6Gj9Ka0dJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs:\n",
      "tensor([[-0.5599],\n",
      "        [-0.7804],\n",
      "        [ 1.2618],\n",
      "        [ 0.7351],\n",
      "        [ 0.3134],\n",
      "        [ 0.1207],\n",
      "        [ 0.4455],\n",
      "        [ 0.7361],\n",
      "        [-0.1293],\n",
      "        [ 1.6416]])\n",
      "ys:\n",
      "tensor([[-1.6817],\n",
      "        [ 0.5042],\n",
      "        [ 0.8609],\n",
      "        [ 1.7576],\n",
      "        [-1.8608],\n",
      "        [-0.3588],\n",
      "        [-0.3929],\n",
      "        [-0.2294],\n",
      "        [ 0.4915],\n",
      "        [ 0.8087]])\n"
     ]
    }
   ],
   "source": [
    "example_xs, example_ys = next(iter(tdl))  # grabbing an example batch to print\n",
    "\n",
    "print(\"xs:\", example_xs[:10], sep=\"\\n\")\n",
    "print(\"ys:\", example_ys[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nnqk3mRv8dbW"
   },
   "source": [
    "and, since it's low-dimensional, visualize it\n",
    "and see what we're asking the model to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "33jcHbErbl6Q"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
    "  .plot(x=\"x\", y=\"y\", kind=\"scatter\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA7-4tJJ9fde"
   },
   "source": [
    "Now we're ready to run training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IY910O803oPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 5.011375904083252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c02183f9e6422b9f0158e31a574477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.2114735841751099\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=tdl)\n",
    "\n",
    "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQBXYmLF_GoI"
   },
   "source": [
    "The loss after training should be less than the loss before training,\n",
    "and we can see that our model's predictions line up with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jqcbA91x96-s"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
    "  .plot(x=\"x\", y=\"y\", legend=True, kind=\"scatter\", label=\"data\")\n",
    "\n",
    "inps = torch.arange(-2, 2, 0.5)[:, None]\n",
    "ax.plot(inps, model(inps).detach(), lw=2, color=\"k\", label=\"predictions\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZkpsNfl3P8R"
   },
   "source": [
    "The `Trainer` promises to \"customize every aspect of training via flags\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_Q-c9b62_XFj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Customize every aspect of training via flags.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Trainer.__init__.__doc__.strip().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He-zEwMB_oKH"
   },
   "source": [
    "and they mean _every_ aspect.\n",
    "\n",
    "The cell below prints all of the arguments for the `pl.Trainer` class --\n",
    "no need to memorize or even understand them all now,\n",
    "just skim it to see how many customization options there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8F_rRPL3lfPE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Customize every aspect of training via flags.\n",
      "\n",
      "        Args:\n",
      "\n",
      "            accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\")\n",
      "                as well as custom accelerator instances.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Passing training strategies (e.g., 'ddp') to ``accelerator`` has been deprecated in v1.5.0\n",
      "                    and will be removed in v1.7.0. Please use the ``strategy`` argument instead.\n",
      "\n",
      "            accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
      "                Default: ``None``.\n",
      "\n",
      "            amp_backend: The mixed precision backend to use (\"native\" or \"apex\").\n",
      "                Default: ``'native''``.\n",
      "\n",
      "            amp_level: The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\"\n",
      "                if ``amp_backend`` is set to \"apex\".\n",
      "\n",
      "            auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
      "                trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
      "                set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
      "                To use a different key set a string instead of True with the key name.\n",
      "                Default: ``False``.\n",
      "\n",
      "            auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
      "                finder trying to find the largest batch size that fits into memory.\n",
      "                The result will be stored in self.batch_size in the LightningModule.\n",
      "                Additionally, can be set to either `power` that estimates the batch size through\n",
      "                a power search or `binsearch` that estimates the batch size through a binary search.\n",
      "                Default: ``False``.\n",
      "\n",
      "            auto_select_gpus: If enabled and ``gpus`` or ``devices`` is an integer, pick available\n",
      "                gpus automatically. This is especially useful when\n",
      "                GPUs are configured to be in \"exclusive mode\", such\n",
      "                that only one process at a time can access them.\n",
      "                Default: ``False``.\n",
      "\n",
      "            benchmark: Sets ``torch.backends.cudnn.benchmark``.\n",
      "                Defaults to ``True`` if :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
      "                is ``False``. Overwrite to manually set a different value. Default: ``None``.\n",
      "\n",
      "            callbacks: Add a callback or list of callbacks.\n",
      "                Default: ``None``.\n",
      "\n",
      "            checkpoint_callback: If ``True``, enable checkpointing.\n",
      "                Default: ``None``.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``checkpoint_callback`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please consider using ``enable_checkpointing`` instead.\n",
      "\n",
      "            enable_checkpointing: If ``True``, enable checkpointing.\n",
      "                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
      "                Default: ``True``.\n",
      "\n",
      "            check_val_every_n_epoch: Check val every n train epochs.\n",
      "                Default: ``1``.\n",
      "\n",
      "\n",
      "            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      "                Default: ``os.getcwd()``.\n",
      "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "\n",
      "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "                Default: ``False``.\n",
      "\n",
      "            deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      "                Default: ``False``.\n",
      "\n",
      "            devices: Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`,\n",
      "                based on the accelerator type.\n",
      "\n",
      "            fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      "                of train, val and test to find any bugs (ie: a sort of unit test).\n",
      "                Default: ``False``.\n",
      "\n",
      "            flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``flush_logs_every_n_steps`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please configure flushing directly in the logger instead.\n",
      "\n",
      "            gpus: Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node\n",
      "                Default: ``None``.\n",
      "\n",
      "            gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      "                gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      "                Default: ``None``.\n",
      "\n",
      "            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      "                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      "                be set to ``\"norm\"``.\n",
      "\n",
      "            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      "                the default ``TensorBoardLogger``. ``False`` will disable logging. If multiple loggers are\n",
      "                provided and the `save_dir` property of that logger is not set, local files (checkpoints,\n",
      "                profiler traces, etc.) are saved in ``default_root_dir`` rather than in the ``log_dir`` of any\n",
      "                of the individual loggers.\n",
      "                Default: ``True``.\n",
      "\n",
      "            log_gpu_memory: None, 'min_max', 'all'. Might slow performance.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Deprecated in v1.5.0 and will be removed in v1.7.0\n",
      "                    Please use the ``DeviceStatsMonitor`` callback directly instead.\n",
      "\n",
      "            log_every_n_steps: How often to log within steps.\n",
      "                Default: ``50``.\n",
      "\n",
      "            prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Deprecated in v1.5.0 and will be removed in v1.7.0\n",
      "                    Please set ``prepare_data_per_node`` in ``LightningDataModule`` and/or\n",
      "                    ``LightningModule`` directly instead.\n",
      "\n",
      "            process_position: Orders the progress bar when running multiple models on same machine.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``process_position`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``process_position``\n",
      "                    directly to the Trainer's ``callbacks`` argument instead.\n",
      "\n",
      "            progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
      "                Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
      "                a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``progress_bar_refresh_rate`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``refresh_rate``\n",
      "                    directly to the Trainer's ``callbacks`` argument instead. To disable the progress bar,\n",
      "                    pass ``enable_progress_bar = False`` to the Trainer.\n",
      "\n",
      "            enable_progress_bar: Whether to enable to progress bar by default.\n",
      "                Default: ``False``.\n",
      "\n",
      "            profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      "                Default: ``None``.\n",
      "\n",
      "            overfit_batches: Overfit a fraction of training data (float) or a set number of batches (int).\n",
      "                Default: ``0.0``.\n",
      "\n",
      "            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      "                Default: ``None``.\n",
      "\n",
      "            precision: Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).\n",
      "                Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
      "                Default: ``32``.\n",
      "\n",
      "            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      "                If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      "                To enable infinite training, set ``max_epochs = -1``.\n",
      "\n",
      "            min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      "\n",
      "            max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      "                and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      "                ``max_epochs`` to ``-1``.\n",
      "\n",
      "            min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      "\n",
      "            max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      "                The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      "                :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      "                :class:`datetime.timedelta`.\n",
      "\n",
      "            num_nodes: Number of GPU nodes for distributed training.\n",
      "                Default: ``1``.\n",
      "\n",
      "            num_processes: Number of processes for distributed training with ``accelerator=\"cpu\"``.\n",
      "                Default: ``1``.\n",
      "\n",
      "            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      "                Set it to `-1` to run all batches in all validation dataloaders.\n",
      "                Default: ``2``.\n",
      "\n",
      "            reload_dataloaders_every_n_epochs: Set to a non-negative integer to reload dataloaders every n epochs.\n",
      "                Default: ``0``.\n",
      "\n",
      "            replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
      "                will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
      "                train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
      "                you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
      "\n",
      "            resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
      "                no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,\n",
      "                training will start from the beginning of the next epoch.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``resume_from_checkpoint`` is deprecated in v1.5 and will be removed in v2.0.\n",
      "                    Please pass the path to ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
      "\n",
      "            strategy: Supports different training strategies with aliases\n",
      "                as well custom strategies.\n",
      "                Default: ``None``.\n",
      "\n",
      "            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      "                Default: ``False``.\n",
      "\n",
      "            terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
      "                end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Trainer argument ``terminate_on_nan`` was deprecated in v1.5 and will be removed in 1.7.\n",
      "                    Please use ``detect_anomaly`` instead.\n",
      "\n",
      "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "                Default: ``False``.\n",
      "\n",
      "            tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on (1)\n",
      "                Default: ``None``.\n",
      "\n",
      "            ipus: How many IPUs to train on.\n",
      "                Default: ``None``.\n",
      "\n",
      "            track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using\n",
      "                Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\n",
      "                Default: ``-1``.\n",
      "\n",
      "            val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      "                after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      "                batches.\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            enable_model_summary: Whether to enable model summarization by default.\n",
      "                Default: ``True``.\n",
      "\n",
      "            weights_summary: Prints a summary of the weights when training begins.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``weights_summary`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    To disable the summary, pass ``enable_model_summary = False`` to the Trainer.\n",
      "                    To customize the summary, pass :class:`~pytorch_lightning.callbacks.model_summary.ModelSummary`\n",
      "                    directly to the Trainer's ``callbacks`` argument.\n",
      "\n",
      "            weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
      "                for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
      "                stored in a different place than the logs written in `default_root_dir`.\n",
      "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "                Defaults to `default_root_dir`.\n",
      "\n",
      "                .. deprecated:: v1.6\n",
      "                    ``weights_save_path`` has been deprecated in v1.6 and will be removed in v1.8. Please pass\n",
      "                    ``dirpath`` directly to the :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      "                    callback.\n",
      "\n",
      "            move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
      "                This can save some gpu memory, but can make training slower. Use with attention.\n",
      "                Default: ``False``.\n",
      "\n",
      "            multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
      "                In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
      "                and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
      "                reload when reaching the minimum length of datasets.\n",
      "                Default: ``\"max_size_cycle\"``.\n",
      "\n",
      "            stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>`_.\n",
      "                Default: ``False``.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``stochastic_weight_avg`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please pass :class:`~pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging`\n",
      "                    directly to the Trainer's ``callbacks`` argument instead.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(pl.Trainer.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X8dGmR53kYU"
   },
   "source": [
    "It's probably easier to read them on the documentation website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cqUj6MxRkppr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pytorch-lightning.readthedocs.io/en/1.6.3/common/trainer.html'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_docs_link = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/trainer.html\"\n",
    "trainer_docs_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T8XMYvr__Y5"
   },
   "source": [
    "# Training with PyTorch Lightning in the FSDL Codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CtaPliTAxy3"
   },
   "source": [
    "The `LightningModule`s in the FSDL codebase\n",
    "are stored in the `lit_models` submodule of the `text_recognizer` module.\n",
    "\n",
    "For now, we've just got some basic models.\n",
    "We'll add more as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NMe5z1RSAyo_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  __pycache__  base.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls text_recognizer/lit_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZTYmIHbBu7g"
   },
   "source": [
    "We also have a folder called `training` now.\n",
    "\n",
    "This contains a script, `run_experiment.py`,\n",
    "that is used for running training jobs.\n",
    "\n",
    "In case you want to play around with the training code\n",
    "in a notebook, you can also load it as a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DRz9GbXzNJLM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  __pycache__  run_experiment.py  util.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Im9vLeyqBv_h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment-running framework. \n",
      "    Run an experiment.\n",
      "\n",
      "    Sample command:\n",
      "    ```\n",
      "    python training/run_experiment.py --max_epochs=3 --gpus='0,' --num_workers=20 --model_class=MLP --data_class=MNIST\n",
      "    ```\n",
      "\n",
      "    For basic help documentation, run the command\n",
      "    ```\n",
      "    python training/run_experiment.py --help\n",
      "    ```\n",
      "\n",
      "    The available command line args differ depending on some of the arguments, including --model_class and --data_class.\n",
      "\n",
      "    To see which command line args are available and read their documentation, provide values for those arguments\n",
      "    before invoking --help, like so:\n",
      "    ```\n",
      "    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import training.run_experiment\n",
    "\n",
    "\n",
    "print(training.run_experiment.__doc__, training.run_experiment.main.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2hcAXqHAV0v"
   },
   "source": [
    "We build the `Trainer` from command line arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yi50CDZul7Mm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks, logger=logger)\r\n"
     ]
    }
   ],
   "source": [
    "# how the trainer is initialized in the training script\n",
    "!grep \"pl.Trainer.from\" training/run_experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZQheYJyAxlh"
   },
   "source": [
    "so all the configuration flexibility and complexity of the `Trainer`\n",
    "is available via the command line.\n",
    "\n",
    "Docs for the command line arguments for the trainer are accessible with `--help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XlSmSyCMAw7Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl.Trainer:\r\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\r\n",
      "                        experiment tracking. A ``True`` value uses the default\r\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\r\n",
      "                        If multiple loggers are provided and the `save_dir`\r\n",
      "                        property of that logger is not set, local files\r\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\r\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\r\n",
      "                        any of the individual loggers. Default: ``True``.\r\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\r\n",
      "                        If ``True``, enable checkpointing. Default: ``None``.\r\n",
      "                        .. deprecated:: v1.5 ``checkpoint_callback`` has been\r\n",
      "                        deprecated in v1.5 and will be removed in v1.7. Please\r\n",
      "                        consider using ``enable_checkpointing`` instead.\r\n",
      "  --enable_checkpointing [ENABLE_CHECKPOINTING]\r\n",
      "                        If ``True``, enable checkpointing. It will configure a\r\n",
      "                        default ModelCheckpoint callback if there is no user-\r\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\r\n",
      "                        ng.trainer.trainer.Trainer.callbacks`. Default:\r\n",
      "                        ``True``.\r\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\r\n",
      "                        Default path for logs and weights when no\r\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\r\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\r\n",
      "                        or 'hdfs://path/'\r\n"
     ]
    }
   ],
   "source": [
    "# displays the first few flags for controlling the Trainer from the command line\n",
    "!python training/run_experiment.py --help | grep \"pl.Trainer\" -A 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIZ_VRPcNMsM"
   },
   "source": [
    "We'll use `run_experiment` in\n",
    "[Lab 02b](http://fsdl.me/lab02b-colab)\n",
    "to train convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0siaL4Qumc_"
   },
   "source": [
    "# Extra Goodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkQSPnxQDBF6"
   },
   "source": [
    "The `LightningModule` and the `Trainer` are the minimum amount you need\n",
    "to get started with PyTorch Lightning.\n",
    "\n",
    "But they aren't all you need.\n",
    "\n",
    "There are many more features built into Lightning and its ecosystem.\n",
    "\n",
    "We'll cover three more here:\n",
    "- `pl.LightningDataModule`s, for organizing dataloaders and handling data in distributed settings\n",
    "- `pl.Callback`s, for adding \"optional\" extra features to model training\n",
    "- `torchmetrics`, for efficiently computing and logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOYHSLw_D8Zy"
   },
   "source": [
    "## `pl.LightningDataModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpjTNGzREIpl"
   },
   "source": [
    "Where the `LightningModule` organizes our model and its optimizers,\n",
    "the `LightningDataModule` organizes our dataloading code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_KkQ0iOWKD7"
   },
   "source": [
    "The class-level docstring explains the concept\n",
    "behind the class well\n",
    "and lists the main methods to be over-ridden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "IFTWHdsFV5WG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
      "    advantage is consistent data splits, data preparation and transforms across models.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        class MyDataModule(LightningDataModule):\n",
      "            def __init__(self):\n",
      "                super().__init__()\n",
      "            def prepare_data(self):\n",
      "                # download, split, etc...\n",
      "                # only called on 1 GPU/TPU in distributed\n",
      "            def setup(self, stage):\n",
      "                # make assignments here (val/train/test split)\n",
      "                # called on every process in DDP\n",
      "            def train_dataloader(self):\n",
      "                train_split = Dataset(...)\n",
      "                return DataLoader(train_split)\n",
      "            def val_dataloader(self):\n",
      "                val_split = Dataset(...)\n",
      "                return DataLoader(val_split)\n",
      "            def test_dataloader(self):\n",
      "                test_split = Dataset(...)\n",
      "                return DataLoader(test_split)\n",
      "            def teardown(self):\n",
      "                # clean up after fit or test\n",
      "                # called on every process in DDP\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pl.LightningDataModule.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLiacppGB9BB"
   },
   "source": [
    "Let's upgrade our `CorrelatedDataset` from a PyTorch `Dataset` to a `LightningDataModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "m1d62iC6Xv1i"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class CorrelatedDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, size=10_000, train_frac=0.8, batch_size=32):\n",
    "        super().__init__()  # again, mandatory superclass init, as with torch.nn.Modules\n",
    "\n",
    "        # set some constants, like the train/val split\n",
    "        self.size = size\n",
    "        self.train_frac, self.val_frac = train_frac, 1 - train_frac\n",
    "        self.train_indices = list(range(math.floor(self.size * train_frac)))\n",
    "        self.val_indices = list(range(self.train_indices[-1], self.size))\n",
    "\n",
    "        # under the hood, we've still got a torch Dataset\n",
    "        self.dataset = CorrelatedDataset(N=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQf-jUYRCi3m"
   },
   "source": [
    "`LightningDataModule`s are designed to work in distributed settings,\n",
    "where operations that set state\n",
    "(e.g. writing to disk or attaching something to `self` that you want to access later)\n",
    "need to be handled with care.\n",
    "\n",
    "Getting data ready for training is often a very stateful operation,\n",
    "so the `LightningDataModule` provides two separate methods for it:\n",
    "one called `setup` that handles any state that needs to be set up in each copy of the module\n",
    "(here, splitting the data and adding it to `self`)\n",
    "and one called `prepare_data` that handles any state that only needs to be set up in each machine\n",
    "(for example, downloading data from storage and writing it to the local disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mttu--rHX70r"
   },
   "outputs": [],
   "source": [
    "def setup(self, stage=None):  # prepares state that needs to be set for each GPU on each node\n",
    "    if stage == \"fit\" or stage is None:  # other stages: \"test\", \"predict\"\n",
    "        self.train_dataset = torch.utils.data.Subset(self.dataset, self.train_indices)\n",
    "        self.val_dataset = torch.utils.data.Subset(self.dataset, self.val_indices)\n",
    "\n",
    "def prepare_data(self):  # prepares state that needs to be set once per node\n",
    "    pass  # but we don't have any \"node-level\" computations\n",
    "\n",
    "\n",
    "CorrelatedDataModule.setup, CorrelatedDataModule.prepare_data = setup, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh3mZrjwD83Y"
   },
   "source": [
    "We then define methods to return `DataLoader`s when requested by the `Trainer`.\n",
    "\n",
    "To run a testing loop that uses a `LightningDataModule`,\n",
    "you'll also need to define a `test_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xu9Ma3iKYPBd"
   },
   "outputs": [],
   "source": [
    "def train_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(self.train_dataset, batch_size=32)\n",
    "\n",
    "def val_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(self.val_dataset, batch_size=32)\n",
    "\n",
    "CorrelatedDataModule.train_dataloader, CorrelatedDataModule.val_dataloader = train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNodiN6oawX5"
   },
   "source": [
    "Now we're ready to run training using a datamodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JKBwoE-Rajqw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 1.7290900945663452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645772982c074b4c8af27b28fd362dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.0625207424163818\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "datamodule = CorrelatedDataModule()\n",
    "\n",
    "dataset = datamodule.dataset\n",
    "\n",
    "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
    "trainer.fit(model=model, datamodule=datamodule)\n",
    "\n",
    "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw6flh5Jf2ZP"
   },
   "source": [
    "Notice the warning: \"`Skipping val loop.`\"\n",
    "\n",
    "It's being raised because our minimal `LinearRegression` model\n",
    "doesn't have a `.validation_step` method.\n",
    "\n",
    "In the exercises, you're invited to add a validation step and resolve this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJnoFx47ZjBw"
   },
   "source": [
    "In the FSDL codebase,\n",
    "we define the basic functions of a `LightningDataModule`\n",
    "in the `BaseDataModule` and defer details to subclasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "PTPKvDDGXmOr"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.data import BaseDataModule\n",
    "\n",
    "\n",
    "BaseDataModule??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3mRlZecwaKB4"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.data.mnist import MNIST\n",
    "\n",
    "\n",
    "MNIST??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQbMY08qD-hm"
   },
   "source": [
    "## `pl.Callback`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVe7TSNvHK4K"
   },
   "source": [
    "Lightning's `Callback` class is used to add \"nice-to-have\" features\n",
    "to training, validation, and testing\n",
    "that aren't strictly necessary for any model to run\n",
    "but are useful for many models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzU76wgFGw9N"
   },
   "source": [
    "A \"callback\" is a unit of code that's meant to be called later,\n",
    "based on some trigger.\n",
    "\n",
    "It's a very flexible system, which is why\n",
    "`Callback`s are used internally to implement lots of important Lightning features,\n",
    "including some we've already discussed, like `ModelCheckpoint` for saving during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-msDjbKdHTxU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BackboneFinetuning',\n",
       " 'BaseFinetuning',\n",
       " 'Callback',\n",
       " 'DeviceStatsMonitor',\n",
       " 'EarlyStopping',\n",
       " 'GPUStatsMonitor',\n",
       " 'XLAStatsMonitor',\n",
       " 'GradientAccumulationScheduler',\n",
       " 'LambdaCallback',\n",
       " 'LearningRateMonitor',\n",
       " 'ModelCheckpoint',\n",
       " 'ModelPruning',\n",
       " 'ModelSummary',\n",
       " 'BasePredictionWriter',\n",
       " 'ProgressBar',\n",
       " 'ProgressBarBase',\n",
       " 'QuantizationAwareTraining',\n",
       " 'RichModelSummary',\n",
       " 'RichProgressBar',\n",
       " 'StochasticWeightAveraging',\n",
       " 'Timer',\n",
       " 'TQDMProgressBar']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.callbacks.__all__  # builtin Callbacks from Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6WRNXtHHkbM"
   },
   "source": [
    "The triggers, or \"hooks\", here, are specific points in the training, validation, and testing loop.\n",
    "\n",
    "The names of the hooks generally explain when the hook will be called,\n",
    "but you can always check the documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3iHjjnU8Hvgg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooks:\n",
      "\ton_after_backward, on_batch_end, on_batch_start,\n",
      "\ton_before_accelerator_backend_setup, on_before_backward,\n",
      "\ton_before_optimizer_step, on_before_zero_grad, on_configure_sharded_model,\n",
      "\ton_epoch_end, on_epoch_start, on_exception, on_fit_end, on_fit_start,\n",
      "\ton_init_end, on_init_start, on_keyboard_interrupt, on_load_checkpoint,\n",
      "\ton_predict_batch_end, on_predict_batch_start, on_predict_end,\n",
      "\ton_predict_epoch_end, on_predict_epoch_start, on_predict_start,\n",
      "\ton_pretrain_routine_end, on_pretrain_routine_start, on_sanity_check_end,\n",
      "\ton_sanity_check_start, on_save_checkpoint, on_test_batch_end,\n",
      "\ton_test_batch_start, on_test_end, on_test_epoch_end, on_test_epoch_start,\n",
      "\ton_test_start, on_train_batch_end, on_train_batch_start, on_train_end,\n",
      "\ton_train_epoch_end, on_train_epoch_start, on_train_start,\n",
      "\ton_validation_batch_end, on_validation_batch_start, on_validation_end,\n",
      "\ton_validation_epoch_end, on_validation_epoch_start, on_validation_start\n"
     ]
    }
   ],
   "source": [
    "hooks = \", \".join([method for method in dir(pl.Callback) if method.startswith(\"on_\")])\n",
    "print(\"hooks:\", *textwrap.wrap(hooks, width=80), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E2M7O2cGdj7"
   },
   "source": [
    "You can define your own `Callback` by inheriting from `pl.Callback`\n",
    "and over-riding one of the \"hook\" methods --\n",
    "much the same way that you define your own `LightningModule`\n",
    "by writing your own `.training_step` and `.configure_optimizers`.\n",
    "\n",
    "Let's define a silly `Callback` just to demonstrate the idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UodFQKAGEJlk"
   },
   "outputs": [],
   "source": [
    "class HelloWorldCallback(pl.Callback):\n",
    "\n",
    "    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        print(\" hello from the start of the training epoch!\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        print(\" hello from the end of the validation epoch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MU7oIpyEGoaP"
   },
   "source": [
    "This callback will print a message whenever the training epoch starts\n",
    "and whenever the validation epoch ends.\n",
    "\n",
    "Different \"hooks\" have different information directly available.\n",
    "\n",
    "For example, you can directly access the batch information\n",
    "inside the `on_train_batch_start` and `on_train_batch_end` hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "U17Qo_i_GCya"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def on_train_batch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
    "        if random.random() > 0.995:\n",
    "            print(f\" hello from inside the lucky batch, #{batch_idx}!\")\n",
    "\n",
    "\n",
    "HelloWorldCallback.on_train_batch_start = on_train_batch_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVKQXZOwQNGJ"
   },
   "source": [
    "We provide the callbacks when initializing the `Trainer`,\n",
    "then they are invoked during model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-XHXZ64-ETCz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "datamodule = CorrelatedDataModule()\n",
    "\n",
    "trainer = pl.Trainer(  # we instantiate and provide the callback here, but nothing happens yet\n",
    "    max_epochs=10, gpus=int(torch.cuda.is_available()), callbacks=[HelloWorldCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "UEHUUhVOQv6K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be86f4c453114e5f84d8cfc497246fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #45!\n",
      " hello from inside the lucky batch, #63!\n",
      " hello from inside the lucky batch, #110!\n",
      " hello from inside the lucky batch, #140!\n",
      " hello from inside the lucky batch, #162!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #14!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #16!\n",
      " hello from inside the lucky batch, #132!\n",
      " hello from inside the lucky batch, #241!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #29!\n",
      " hello from inside the lucky batch, #93!\n",
      " hello from inside the lucky batch, #108!\n",
      " hello from inside the lucky batch, #238!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #67!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #10!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #80!\n",
      " hello from inside the lucky batch, #87!\n",
      " hello from the start of the training epoch!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #155!\n",
      " hello from the start of the training epoch!\n",
      " hello from inside the lucky batch, #155!\n",
      " hello from inside the lucky batch, #190!\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP2Xj1woFGwG"
   },
   "source": [
    "You can read more about callbacks in the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "COHk5BZvFJN_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pytorch-lightning.readthedocs.io/en/1.6.3/extensions/callbacks.html'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/extensions/callbacks.html\"\n",
    "callback_docs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2K9e44iEGCR"
   },
   "source": [
    "## `torchmetrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO-UIFKyJCqJ"
   },
   "source": [
    "DNNs are also finicky and break silently:\n",
    "rather than crashing, they just start doing the wrong thing.\n",
    "Without careful monitoring, that wrong thing can be invisible\n",
    "until long after it has done a lot of damage to you, your team, or your users.\n",
    "\n",
    "We want to calculate metrics so we can monitor what's happening during training and catch bugs --\n",
    "or even achieve [\"observability\"](https://thenewstack.io/observability-a-3-year-retrospective/),\n",
    "meaning we can also determine\n",
    "how to fix bugs in training just by viewing logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4YMyUI0Jr2f"
   },
   "source": [
    "But DNN training is also performance sensitive.\n",
    "Training runs for large language models have budgets that are\n",
    "more comparable to building an apartment complex\n",
    "than they are to the build jobs of traditional software pipelines.\n",
    "\n",
    "Slowing down training even a small amount can add a substantial dollar cost,\n",
    "obviating the benefits of catching and fixing bugs more quickly.\n",
    "\n",
    "Also implementing metric calculation during training adds extra work,\n",
    "much like the other software engineering best practices which it closely resembles,\n",
    "namely test-writing and monitoring.\n",
    "This distracts and detracts from higher-leverage research work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvWjiHSIxzM"
   },
   "source": [
    "\n",
    "The `torchmetrics` library, which began its life as `pytorch_lightning.metrics`,\n",
    "resolves these issues by providing a `Metric` class that\n",
    "incorporates best performance practices,\n",
    "like smart accumulation across batches and over devices,\n",
    "defines a unified interface,\n",
    "and integrates with Lightning's built-in logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "21y3lgvwEKPC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics:\n",
      "\tfunctional, Accuracy, AUC, AUROC, AveragePrecision, BinnedAveragePrecision,\n",
      "\tBinnedPrecisionRecallCurve, BinnedRecallAtFixedPrecision, BLEUScore,\n",
      "\tBootStrapper, CalibrationError, CatMetric, CHRFScore, CohenKappa,\n",
      "\tConfusionMatrix, CosineSimilarity, TweedieDevianceScore, ExplainedVariance,\n",
      "\tExtendedEditDistance, F1, F1Score, FBeta, FBetaScore, HammingDistance, Hinge,\n",
      "\tHingeLoss, JaccardIndex, KLDivergence, MatthewsCorrcoef, MatthewsCorrCoef,\n",
      "\tMaxMetric, MeanAbsoluteError, MeanAbsolutePercentageError, MeanMetric,\n",
      "\tMeanSquaredError, MeanSquaredLogError, Metric, MetricCollection, MetricTracker,\n",
      "\tMinMaxMetric, MinMetric, MultioutputWrapper,\n",
      "\tMultiScaleStructuralSimilarityIndexMeasure, PearsonCorrcoef, PearsonCorrCoef,\n",
      "\tPermutationInvariantTraining, PIT, Precision, PrecisionRecallCurve, PSNR,\n",
      "\tPeakSignalNoiseRatio, R2Score, Recall, RetrievalFallOut, RetrievalHitRate,\n",
      "\tRetrievalMAP, RetrievalMRR, RetrievalNormalizedDCG, RetrievalPrecision,\n",
      "\tRetrievalRecall, RetrievalRPrecision, ROC, SacreBLEUScore, SDR,\n",
      "\tSignalDistortionRatio, ScaleInvariantSignalDistortionRatio, SI_SDR, SI_SNR,\n",
      "\tScaleInvariantSignalNoiseRatio, SignalNoiseRatio, SNR, SpearmanCorrcoef,\n",
      "\tSpearmanCorrCoef, Specificity, SQuAD, SSIM, StructuralSimilarityIndexMeasure,\n",
      "\tStatScores, SumMetric, SymmetricMeanAbsolutePercentageError,\n",
      "\tTranslationEditRate, WER, WordErrorRate, CharErrorRate, MatchErrorRate,\n",
      "\tWordInfoLost, WordInfoPreserved\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "tm_version = torchmetrics.__version__\n",
    "print(\"metrics:\", *textwrap.wrap(\", \".join(torchmetrics.__all__), width=80), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TuPZkV1gfFE"
   },
   "source": [
    "Like the `LightningModule`, `torchmetrics.Metric` inherits from `torch.nn.Module`.\n",
    "\n",
    "That's because metric calculation, like module application, is typically\n",
    "1) an array-heavy computation that\n",
    "2) relies on persistent state\n",
    "(parameters for `Module`s, running values for `Metric`s) and\n",
    "3) benefits from acceleration and\n",
    "4) can be distributed over devices and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "leiiI_QDS2_V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issubclass(torchmetrics.Metric, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy8MF2taP8MV"
   },
   "source": [
    "Documentation for the version of `torchmetrics` we're using can be found here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LN4ashooP_tM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://torchmetrics.readthedocs.io/en/v0.7.3/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchmetrics_docs_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/\"\n",
    "torchmetrics_docs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aycHhZNXwjr"
   },
   "source": [
    "In the `BaseLitModel`,\n",
    "we use the `torchmetrics.Accuracy` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Vyq4IjmBXzTv"
   },
   "outputs": [],
   "source": [
    "BaseLitModel.__init__??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPoTH50YfkMF"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD_6PVAeflWw"
   },
   "source": [
    "###  Add a `validation_step` to the `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.LightningModule??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "5KKbAN9eK281"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def validation_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "    x, y = batch\n",
    "    loss = torch.mean(torch.square(model(x) - y)).item()\n",
    "    pred = ...\n",
    "    self.log(\"val_loss\", loss, prog_bar=True)\n",
    "    return{\"val_loss\": loss, \"pred\": pred}\n",
    "\n",
    "\n",
    "LinearRegression.validation_step = validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "AnPPHAPxFCEv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476c5888145543dcb2164df744c480ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "datamodule = CorrelatedDataModule()\n",
    "\n",
    "dataset = datamodule.dataset\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
    "# if you code is working, you should see results for the validation loss in the output\n",
    "trainer.fit(model=model, datamodule=datamodule)\n",
    "#trainer.validate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u42zXktOFDhZ"
   },
   "source": [
    "###  Add a `test_step` to the `LinearRegression` class and a `test_dataloader` to the `CorrelatedDataModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "cbWfqvumFESV"
   },
   "outputs": [],
   "source": [
    "def test_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "    x, y = batch\n",
    "    loss = torch.mean(torch.square(model(x) - y)).item()\n",
    "    pred = ...\n",
    "    return{\"test_loss\": loss, \"pred\": pred}\n",
    "\n",
    "LinearRegression.test_step = test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "pB96MpibLeJi"
   },
   "outputs": [],
   "source": [
    "class CorrelatedDataModuleWithTest(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, N=10_000, N_test=10_000):  # reimplement __init__ here\n",
    "        super().__init__()  # don't forget this!\n",
    "        self.dataset = None\n",
    "        self.test_dataset = None  # define a test set -- another sample from the same distribution\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
    "        pass  # create a dataloader for the test set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class CorrelatedDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, size=10_000, train_frac=0.8, batch_size=32):\n",
    "        super().__init__()  # again, mandatory superclass init, as with torch.nn.Modules\n",
    "\n",
    "        # set some constants, like the train/val split\n",
    "        self.size = size\n",
    "        self.train_frac, self.val_frac = train_frac, 1 - train_frac\n",
    "        self.train_indices = list(range(math.floor(self.size * train_frac)))\n",
    "        self.val_indices = list(range(self.train_indices[-1], self.size))\n",
    "\n",
    "        # under the hood, we've still got a torch Dataset\n",
    "        self.dataset = CorrelatedDataset(N=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "1jq3dcugMMOu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/terps/mambaforge/envs/fsdl/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:378: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "  rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "datamodule = CorrelatedDataModuleWithTest()\n",
    "\n",
    "dataset = datamodule.dataset\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
    "\n",
    "# we run testing without fitting here\n",
    "trainer.test(model=model, datamodule=datamodule)  # if your code is working, you should see performance on the test set here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHg4MKmJPla6"
   },
   "source": [
    "###  Make a version of the `LinearRegression` class that calculates the `ExplainedVariance` metric during training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_1AKGWRR2ai"
   },
   "source": [
    "The \"variance explained\" is a useful metric for comparing regression models --\n",
    "its values are interpretable and comparable across datasets, unlike raw loss values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLecK4CsQWKk"
   },
   "source": [
    "Read the \"TorchMetrics in PyTorch Lightning\" guide for details on how to\n",
    "add metrics and metric logging\n",
    "to a `LightningModule`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "cWy0HyG4RYnX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://torchmetrics.readthedocs.io/en/v0.7.3/pages/lightning.html'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchmetrics_guide_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/pages/lightning.html\"\n",
    "torchmetrics_guide_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoSQ3y6sSTvP"
   },
   "source": [
    "And check out the docs for `ExplainedVariance` to see how it's calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "GpGuRK2FRHh1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Computes `explained variance`_:\n",
      "\n",
      "    .. math:: \\text{ExplainedVariance} = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)}\n",
      "\n",
      "    Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a\n",
      "    tensor of predictions.\n",
      "\n",
      "    Forward accepts\n",
      "\n",
      "    - ``preds`` (float tensor): ``(N,)`` or ``(N, ...)`` (multioutput)\n",
      "    - ``target`` (long tensor): ``(N,)`` or ``(N, ...)`` (multioutput)\n",
      "\n",
      "    In the case of multioutput, as default the variances will be uniformly\n",
      "    averaged over the additional dimensions. Please see argument `multioutput`\n",
      "    for changing this behavior.\n",
      "\n",
      "    Args:\n",
      "        multioutput:\n",
      "            Defines aggregation in the case of multiple output scores. Can be one\n",
      "            of the following strings (default is `'uniform_average'`.):\n",
      "\n",
      "            * `'raw_values'` returns full set of scores\n",
      "            * `'uniform_average'` scores are uniformly averaged\n",
      "            * `'variance_weighted'` scores are weighted by their individual variances\n",
      "\n",
      "        compute_on_step:\n",
      "            Forward only calls ``update()`` and return None if this is set to False.\n",
      "        dist_sync_on_step:\n",
      "            Synchronize metric state across processes at each ``forward()``\n",
      "            before returning the value at the step.\n",
      "        process_group:\n",
      "            Specify the process group on which synchronization is called.\n",
      "\n",
      "    Raises:\n",
      "        ValueError:\n",
      "            If ``multioutput`` is not one of ``\"raw_values\"``, ``\"uniform_average\"`` or ``\"variance_weighted\"``.\n",
      "\n",
      "    Example:\n",
      "        >>> from torchmetrics import ExplainedVariance\n",
      "        >>> target = torch.tensor([3, -0.5, 2, 7])\n",
      "        >>> preds = torch.tensor([2.5, 0.0, 2, 8])\n",
      "        >>> explained_variance = ExplainedVariance()\n",
      "        >>> explained_variance(preds, target)\n",
      "        tensor(0.9572)\n",
      "\n",
      "        >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])\n",
      "        >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])\n",
      "        >>> explained_variance = ExplainedVariance(multioutput='raw_values')\n",
      "        >>> explained_variance(preds, target)\n",
      "        tensor([0.9677, 1.0000])\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torchmetrics.ExplainedVariance.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EAtpWXrSVR1"
   },
   "source": [
    "You'll want to start the `LinearRegression` class over from scratch,\n",
    "since the `__init__` and `{training, validation, test}_step` methods need to be rewritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "rGtWt3_5SYTn"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFWNr1SfS5-r"
   },
   "source": [
    "You can test your code by running fitting and testing.\n",
    "\n",
    "To see whether it's working,\n",
    "[call `self.log` inside the `_step` methods](https://torchmetrics.readthedocs.io/en/v0.7.1/pages/lightning.html)\n",
    "with the\n",
    "[keyword argument `prog_bar=True`](https://pytorch-lightning.readthedocs.io/en/1.6.1/api/pytorch_lightning.core.LightningModule.html#pytorch_lightning.core.LightningModule.log).\n",
    "You should see the explained variance show up in the output alongside the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Jse95DGCS6gR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade25fac3b5041889f665e4811fa5253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "datamodule = CorrelatedDataModule()\n",
    "\n",
    "dataset = datamodule.dataset\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
    "\n",
    "# if your code is working, you should see explained variance in the progress bar/logs\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab02a_lightning.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "fsdl",
   "language": "python",
   "name": "fsdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
